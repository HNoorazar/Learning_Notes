\chapter{Deep Learning}
\label{chap:Deep_Learning}

\section{Introduction}
\label{sec:Deep_Learning_introduction}


\subsection{Basics}\label{deepLearningBasics}
Figure~\ref{fig:ActivationFunctions} shows
examples of activation functions. ReLu is the most
widely used one.\marginnote[-1\baselineskip]{This is from Deep Learning Specialization course. The first course (Neural Networks and Deep Learning), week 3.} In order to compute interesting features we need to use
nonlinear activation functions.
\iffalse
\begin{textfigure}[h!]
  \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.5\textwidth]{./00_figures/sigmoid_function_1}%
    \includegraphics[width=0.5\textwidth]{./00_figures/tanh_function_1}
\caption{Examples of activation functions. Use ReLu.}
\label{fig:ActivationFunctions1}
\end{textfigure}
\fi

\begin{figure}
\centering
\subfloat[]{\includegraphics[width=0.5\textwidth]{00_figures/sigmoid_function_1}}
\subfloat[]{\includegraphics[width=0.5\textwidth]{00_figures/tanh_function_1}}
\subfloat[]{\includegraphics[width=0.5\textwidth]{00_figures/reLu_1}}
\caption[][11\baselineskip]{Activation function examples.
\label{fig:ActivationFunctions}}
\end{figure}

The weights has to be initialized randomly
in order to break the symmetry! If all weights are zeros
then we do not compute anything. If all weights
are identical, then each neuron of a given layer
is computing the same thing. The initial weights
better be small. If initial weights are large, then
the activation functions (such as sigmoid or tanh) will be at the far ends
of the spectrum where derivatives are close to zero
and consequently, gradient descent will be slow.

\section{Improving Neural Networks: Hyper-parameter tuning, Regularization, and Optimization}\label{ImprovingNeuralNetworks}

\begin{description}
\item [Train, Validation, and Test Sets] When the dataset is small (e.g. less than 1000), one can divide the data into
training/validation/test set like so 70\%, 30\% (or 60\%, 20\%, 20\%).
However, in the big data era with large datasets (e.g. 1,000,000 data)
the percentage of validation/test set is smaller as percentage (e.g. 98\%, 1\%, 1\%).

\item [Data Distribution] Shuffle the data so that the (training,) validation and test sets come from the same distribution. 

\end{description}

Below is a flowchart of a basic recipe for machine learning.
In deep learning and era of big data there is not a trade-off between
variance and bias. One can use a big network to reduce bias 
without necessarily hurting variance. In this case regularization
is helpful to take care of the variance. One can get  a large number of data
to reduce variance without hurting the bias.\\

\begin{tikzpicture}[node distance=2cm]
\node (HighBias) [startstop] {High Bias};
\node (withBias) [io, right of=HighBias, xshift=1cm] {Yes};
\node (withBiasSolution) [process, right of=withBias, xshift=3cm] {Bigger Network,
Train Longer, (architecture)};

\node (noBias) [io, below of=HighBias] {No};
\node (HighVariance) [startstop, below of=noBias] {High Variance};
\node (withVariance) [io, right of=HighVariance, xshift=1cm] {Yes};
\node (withVarSolution) [process, right of=withVariance, xshift=3cm] {More Data, Regularization, (architecture)};

\node (noVariance) [io, below of=HighVariance] {No};
\node (Done) [decision, below of=noVariance] {Done};

\draw [arrow] (HighBias) -- (noBias);
\draw [arrow] (HighBias) -- (withBias);
\draw [arrow] (withBias) -- (withBiasSolution);
\draw [arrow] (noBias) -- (HighVariance);
\draw [arrow] (HighVariance) -- (withVariance);
\draw [arrow] (HighVariance) -- (noVariance);
\draw [arrow] (withVariance) -- (withVarSolution);
\draw [arrow] (noVariance) -- (Done);

\end{tikzpicture}

\subsection{Regularization}\label{Regularization}
Having high variance (overfitting) can be taken care of by regularization
or obtaining more data which is not always possible. Below is a list of
regularization techniques.
\begin{enumerate}
\item L2 regularization
\item Dropout regularization
\item Obtaining more data (data augmentation; rotation, reflection, cropping)
\item Early stopping; plot train and validation errors.
\end{enumerate}


One of the ways of regularization is similar to L2-regularization (reffered to as
\emph{weight decay} here):
\begin{equation}
J(w^{[1]}, b^{[1]}, \dots, w^{[l]}, b^{[l]}) = 
\frac{1}{m} \sum_{i=1}^m \ell(\hat y^{(i)} - y^{(i)}) + 
 \frac{\lambda}{2m}\sum_{l=1}^{L} ||W^{[l]}||_F^2
\end{equation}

Then we have

\begin{equation}
dW^{[l]} = \frac{\partial J}{\partial W^{[l]}} = (from~backprop) + \frac{\lambda}{m} W^{[l]}
\end{equation}

and the updates will be as follows:
\begin{gather} % gather and aligned leads to having one label for eq.
\begin{aligned} 
W^{[l]} &= W^{[l]} - \alpha dW^{[l]} \\
           &= W^{[l]} - \alpha \left[(from~backprop) + \frac{\lambda}{m} W^{[l]} \right] \\ 
           &= W^{[l]} - \alpha \frac{\lambda}{m} W^{[l]} - \alpha (from~backprop) 
\end{aligned} 
\end{gather} %
The first two terms in above equation is referred to as
\emph{weight decay}.\\

In drop out regularization, (for each training example) at every training step, 
we train the model on a smaller network where the smaller
network is obtained by tossing neurons randomly. The
structure of the network will be different for each training example.
For test examples there will not be any drop out.

One can have different probability of dropping out
for each layer. Specifically, the layers with large number
of weights can enjoy a higher drop out rate/probability.

In computer vision, where there are not a lot of data, the
overfitting is a problem and consequently drop out method
is often used.\\

Early stopping is a technique for regularization. However,
it is interfering with the task of optimizing the cost function.
Let me elaborate. In ML it is a good practice to think and work
on a given task (e.g. minimizing the cost function for which we have
a set of tools, or, reducing the overfitting) one at a time. However,
early stopping which is used for avoiding overfitting
couples the problem at hand with the task of minimizing the cost function.

\subsection{Normalizing the Inputs}\label{NormalizingtheInputs}
Normalizing the inputs helps with the training process.

\subsection{Vanishing/Exploding Gradients}\label{Vanishing/ExplodingGradients}
A partial solution for avoiding the problem of vanishing/exploding
the weights one can be careful with the initialization of the weights.
Let the number of ingoing features of a neuron on layer $l$ be $n^{[l-1]}$.
For that layer we have $z = w_1 x_1 + \dots + w_n{^{[l-1]}} x_n{^{[l-1]}}$.
Consequently, one can do the following for initialization of
the weights:
\[W^{[l]} = np.random.randn(proper~shape) * np.sqrt(1/n^{[l-1]})\]
If ReLu is the activation function when one can/is advised to replace 
$np.sqrt(1/n^{[l-1]})$ with $np.sqrt(2/n^{[l-1]})$.

\subsection{Mini-batch Gradient Descent}
Rather than using the whole training data 
$\mathbf{X} \in \mathbb{R}^{p \times N}$ to 
take an step of gradient descent, take small subsets of $\mathbf{X}; \mathbf{X_{mini}} \in \mathbb{R}^{p \times 1000}$
at every iteration as the training set.
\begin{marginfigure}% [-1\baselineskip]  % Vertical offset of 1 line,
  \centering
  %\captionsetup{justification=centering}
  \includegraphics[width=\textwidth]{00_figures/costFunction}
  \caption{Using mini-batch makes the cost function not to be monotonically decreasing.}
  \label{fig:costFunction}
\end{marginfigure}
One epoch in deep learning means making one
pass (forward and backward propagation) through  
the training set. If we divide the training set into 500
mini-batches, with one epoch we take 500 steps
for gradient descent.

If mini-batch size is 1 then we get stochastic gradient descent.
Here we lose vectorization advantage. Typical sizes for mini-batch 
are 64, 128, 256, 512. 


\subsection{Fast Optimization Methods}
There are methods that converge faster than mini-batch gradient descent:
\begin{itemize}
\item Gradient descent with momentum,
\item RMSprop,
\item Adam optimization algorithm.
\end{itemize}

Let $\{a_t\}$ be a sequence. In order to get a smooth version of the
sequence we can compute $v_t = \frac{\beta v_{t-1} + (1-\beta) a_t}{1-\beta^t}$
that is called \emph{exponentially weighted averages}\marginnote[-1\baselineskip]{This is from Deep Learning Specialization course. The second course (improving the neural networks), week 2.}. The denominator
corrects for the bias we have at the beginning of the sequence. This is the basis
for the next optimization algorithms.

\textbf{Gradient descent with momentum} improves the convergence of
gradient descent. It can prevent oscillation in some directions.
In this method we will have the following for iteration $t$:

\begin{tcolorbox}
%  \textbf{Algorithm for gradient descent with momentum is given below}
  \begin{algorithm}[H]
\SetAlgoLined
\KwResult{Faster convergence}
$\mathbf{V_{d_w}} = \mathbf{0}, \mathbf{V_{d_b}} = \mathbf{0}  $\;
 \While{at iteration $t$}{
  compute $\mathbf{dW, db}$ on current (mini-)batch\;
  $\mathbf{V}_{d_w} = \beta \mathbf{V}_{d_w} + (1-\beta) {d\mathbf{W}} $\;
  $\mathbf{V}_{d_b} = \beta \mathbf{V}_{d_b} + (1-\beta) {d\mathbf{b}} $\;
  $\mathbf{W} = \mathbf{W} - \alpha \mathbf{V}_{d_w}$\;
  $\mathbf{b} = \mathbf{b} - \alpha \mathbf{V}_{d_b}$\;
 }
 \caption{gradient descent with momentum}
\end{algorithm}
Usually $\beta=0.9$ is a good option.
\end{tcolorbox}

\textbf{RMSprop} is another optimization algorithm that
similarly tries to minimize oscillations:
\begin{tcolorbox}
%  \textbf{Algorithm for gradient descent with momentum is given below}
  \begin{algorithm}[H]
\SetAlgoLined
\KwResult{Faster convergence}
$\mathbf{s_{d_w}} = \mathbf{0}, \mathbf{s_{d_b}} = \mathbf{0}  $\;
 \While{While condition}{
  compute $\mathbf{dW, db}$ on current (mini-)batch\;
  $\mathbf{S}_{d_w} = \beta \mathbf{S}_{d_w} + (1-\beta) {d\mathbf{W}}^2 $\;
  $\mathbf{S}_{d_b}  = \beta \mathbf{S}_{d_b} + (1-\beta) {d\mathbf{b}}^2 $\;
  $\mathbf{W} =\mathbf{W} - \alpha \frac{d\mathbf{W}}{\epsilon + \sqrt{s_{d_w}}}$\;
  $\mathbf{b} = \mathbf{b} - \alpha \frac{d\mathbf{b}}{\epsilon +  \sqrt{\mathbf{S}_{d_b}}}$\;
   }
 \caption{RMSprop algorithm}
\end{algorithm}
${d\mathbf{W}}^2$ and ${d\mathbf{b}}^2$ are element-wise. $\epsilon$ is to avoid dividing by zero.\\
Usually $\beta=0.9$ is a good option.
\end{tcolorbox}
\marginnote[-1\baselineskip]{The video says $d\mathbf{W}$ and $d\mathbf{b}$
however, I am suspicious it should be $\mathbf{S}_{d_w}$ and $\mathbf{S}_{d_b}$.}

\textbf{Adams optimization} combines gradient descent with momentum
and the RMSprop:
\begin{tcolorbox}
%  \textbf{Algorithm for gradient descent with momentum is given below}
  \begin{algorithm}[H]
\SetAlgoLined
\KwResult{Faster convergence}
$\mathbf{S_{d_w}}=\mathbf{0},~ \mathbf{V_{d_w}}=\mathbf{0},~ \mathbf{S_{d_b}} = \mathbf{0},~\mathbf{V_{d_b}} = \mathbf{0}  $\;
 \While{at iteration $t$}{
 
  compute $d\mathbf{W}, d\mathbf{b}$ on current (mini-)batch\;
  $\mathbf{V}_{d_w} = \beta_1 \mathbf{V}_{d_w} + (1-\beta_1) {d\mathbf{W}}$\;
  $\mathbf{S}_{d_w} = \beta_2 \mathbf{S}_{d_w} + (1-\beta_2) {d\mathbf{W}}^2$\;
  \vspace{.2in}
  $\mathbf{V}_{d_b}  = \beta_1 \mathbf{V}_{d_b} + (1-\beta_1) {d\mathbf{b}}$\;
  $\mathbf{S}_{d_b}  = \beta_2 \mathbf{S}_{d_b} + (1-\beta_2) {d\mathbf{b}}^2$\;
  \vspace{.2in}
  $\mathbf{V}_{d_w}^{corrected} = \mathbf{V}_{d_w}/(1-\beta_1^t),
  \mathbf{V}_{d_b}^{corrected} = \mathbf{V}_{d_b}/(1-\beta_1^t)$\;
  $\mathbf{S}_{d_w}^{corrected} = \mathbf{S}_{d_w}/(1-\beta_2^t), ~~
  \mathbf{S}_{d_b}^{corrected} = \mathbf{S}_{d_b}/(1-\beta_2^t)$\;
  \vspace{.2in}
  
  $\mathbf{W} = \mathbf{W} - \alpha \frac{\mathbf{V}_{d_w}^{corrected}}{\epsilon + \sqrt{\mathbf{S}_{d_w}^{corrected}}}$\;
  $\mathbf{b} =  \mathbf{b} - \alpha \frac{\mathbf{V}_{d_b}^{corrected}}{\epsilon +  \sqrt{\mathbf{S}_{d_b}^{corrected}}}$\
 }
 \caption{Adams algorithm}
\end{algorithm}
 Usually $\beta_1=0.9$, $\beta_2=0.999$ are a good options, and $\alpha$ needs
 to be tuned.
\end{tcolorbox}



