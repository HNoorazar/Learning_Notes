\section{Basics}
\label{sec:Fundamentals_Basics}

\begin{deff}{}\label{def:ChiSquareDef}
If $Z_1, \dots, Z_k$ are independent, standard normal random 
variables, then the sum of their squares, is distributed according 
to the chi-squared distribution with k degrees of freedom
\[Q = \sum_{i=1}^K Z_i^2\]
This is denoted by $Q \sim \chi^2_K$.
\end{deff}{}
\noindent The chi-squared	 distribution is used primarily in hypothesis testing, and to a lesser extent for confidence intervals for population variance when the underlying distribution is normal. Unlike more widely known distributions such as the normal distribution and the exponential distribution, the chi-squared distribution is not as often applied in the direct modeling of natural phenomena. It arises in the following hypothesis tests, among others [Wikipedia]:
\begin{itemize}
\item Chi-squared test of independence in contingency tables
\item Chi-squared test of goodness of fit of observed data to hypothetical distributions
\item Likelihood-ratio test for nested models
\end{itemize}


\begin{deff}{}\label{def:UnivariateCovarianceDef}
Covariance is a measure of the joint variability of two 
random variables.

\begin{equation}\label{eq:covarianceDef}
\begin{aligned}
cov[X, Y] &= E[(X-\mu_X) (Y - \mu_Y)]\\ 
               &= E[XY] - E[X]E[Y]
\end{aligned}
\end{equation} 

\noindent where $\mu_X \coloneqq E[X]$.
\end{deff}{}

\noindent For discrete random variables we have
\begin{equation}\label{eq:covarianceDiscrete}
\begin{aligned}
cov[X, Y] &= \sum_{i=1}^N p_i (x_i - \mu_x) (y_i - \mu_y)
\end{aligned}
\end{equation} 
\noindent In special case that the (real) random variable pair $(X,Y)$ 
can take on the values $(x_i, y_i)$ for $ i=1, \dots , N$, 
with equal probabilities $p_i=1/N$
\begin{equation}\label{eq:covarianceDiscreteSpecial}
\begin{aligned}
cov[X, Y] &= \frac{1}{N}\sum_{i=1}^N (x_i - \mu_x) (y_i - \mu_y)
\end{aligned}
\end{equation} 
Moreover, $cov (X,X) = var (X) \equiv \sigma^2(X)\equiv \sigma_X^2.$

\iffalse
\begin{cases} 
tr(\mathbf{S}_\lambda)\\
N - tr(2\mathbf{S}_\lambda - \mathbf{S}_\lambda \mathbf{S}_\lambda^T) \\
tr(\mathbf{S}_\lambda \mathbf{S}_\lambda^T)
\end{cases}
\fi

Let $Z_{ij},~i \in \{ 1, 2, \dots,  m\},~j \in \{1, 2, \dots, n \}$
be random variables. Let $\mathbf{Z} = [Z_{ij}]$
and $E[\mathbf{Z}] = [E[Z_{ij}]]$.

\begin{thm}
If $\mathbf{A} \in \mathbb{R}^{l \times m}$,
$\mathbf{B} \in \mathbb{R}^{n \times p}$
$\mathbf{C} \in \mathbb{R}^{l \times p}$ are matrices of constants,
then
\[ E[\mathbf{AZB + C}] = \mathbf{A} E[\mathbf{Z}] \mathbf{B} + \mathbf{C}.\]
\end{thm}\marginnote{For proofs of theorems, at 
least most of the theorems in 
the~\cref{sec:Fundamentals_Basics}, please 
see~\citep{seber2012linear}.}

In the similar vein if $\mathbf{X}$ and $\mathbf{Y}$
are vectors of random variables and $\mathbf{A}$ and 
$\mathbf{B}$ are constant matrices then $E[\mathbf{AX + BY}] = 
\mathbf{A}E[\mathbf{X}] + \mathbf{B}E[\mathbf{Y}]$.


\begin{deff}{}
$cov[\mathbf{X}, \mathbf{Y}] = [cov(X_i, Y_j)]$
\end{deff}{}

\begin{thm}\label{thm:vectorRandCovTHM}
Let $E[\mathbf{X}] = \boldsymbol{\mu}_\mathbf{X}$
and $E[\mathbf{Y}] = \boldsymbol{\mu}_\mathbf{Y}$
then

\[cov[\mathbf{X}, \mathbf{Y}] = E[({\bf X} - \boldsymbol{\mu}_\mathbf{X})({\bf Y} - \boldsymbol{\mu}_\mathbf{Y})^T]. \]
\end{thm}

\begin{deff}{}
Similar to a single random variable, when $\mathbf{Y} = \mathbf{X}$, 
$cov (\mathbf{X}, \mathbf{X})$ is denoted by $var (\mathbf{X})$ and
is called variance (variance-covariance or dispersion) matrix of $\mathbf{X}$:

\begin{equation}\label{eq:varianceCovariance}
\begin{aligned}
var[\mathbf{X}] &= [cov(X_i, X_j)] \\ 
                         &= \begin{bmatrix}
                              var[X_1] & cov[X_1, X_2] & \dots & cov[X_1, X_n]\\
                              cov[X_2, X_1] & var[X_2] & \dots & cov[X_2, X_n]\\
                               \vdots             & \vdots      & \ddots &  \vdots \\
                               cov[X_n, X_1] & cov[X_n, X_2] & \dots & var[X_n]\\
                               \end{bmatrix}. \\ 
\end{aligned}
\end{equation}
\end{deff}{}

From Thm.~\ref{thm:vectorRandCovTHM} we see
\begin{equation}
var[\mathbf{X}] = E[({\bf X} - \boldsymbol{\mu}_\mathbf{X})({\bf X} - \boldsymbol{\mu}_\mathbf{X})^T]
\end{equation}
and by expanding it we get
\begin{equation}
var[\mathbf{X}] = E[{\bf X X}^T] - \boldsymbol{\mu}_\mathbf{X} \boldsymbol{\mu}_\mathbf{X}^T.
\end{equation}
\noindent which we notice is generalization
of univariate case (see Def.~\ref{def:UnivariateCovarianceDef}).

\begin{exm}
Let $\bf a$ be a constant vector. Then $var[{\bf X-a} ] = var[\bf X] - a.$
\end{exm}

\begin{thm}
Let ${\bf X} \in \mathbb{R}^{m}$, 
${\bf Y} \in \mathbb{R}^{n}$ be vectors of random variables while
${\bf A} \in \mathbb{R}^{l \times m}$ and
${\bf B} \in \mathbb{R}^{p \times n}$ are constant matrices.
Then, 
\begin{equation}
cov[{\bf AX, BY}] = {\bf A}  ~ cov[{\bf X}, {\bf Y}]  {\bf B}^T.
\end{equation}
\end{thm}

Of particular importance is $var[{\bf AX}] = {\bf A}~var[{\bf X}] {\bf A}^T$

\begin{thm}
Let ${\bf X}$ be a random variable such that none of its
elements is a linear combination of others. Then, $var[{\bf X}]$ is
a positive definite matrix and therefore invertible.
\end{thm}


\begin{thm}
Let ${\bf X} = [X_i] \in \mathbb{R}^{n}$ be a vector of random variables
and ${\bf A} = {\bf A}^T \in \mathbb{R}^{n \times n}$.
Denote $E[{\bf X}] = \boldsymbol{\mu}_{\bf X}$
and $var[{\bf X}] = \boldsymbol{\Sigma} = [\sigma_{ij}].$
Then, 
\begin{equation}
E[{\bf X}^T{\bf AX} ] = tr({\bf A} \boldsymbol{\Sigma} ) + \boldsymbol{\mu}_{\bf X}^T {\bf A}\boldsymbol{\mu}_{\bf X}
\end{equation}
\end{thm}

Two important special cases of theorem above
are 

\begin{enumerate}
\item Let ${\bf Y = X - b}$ and note that $var [{\bf Y}] = var[{\bf X}]$ then
we have 
\begin{equation}
E[({\bf X-b})^T {\bf A (X-b)}]  = tr({\bf A} \boldsymbol{\Sigma}) + (\boldsymbol{\mu}_{\bf X} - {\bf b} )^T {\bf A (\boldsymbol{\mu}_{\bf X} - {\bf b} )}
\end{equation}

\item If $\boldsymbol{\Sigma} = {\sigma}^2 {\bf I_n}$, then,
$tr({\bf A} \boldsymbol{\Sigma}) = \sigma^2 tr({\bf A} )$. 
Consequently,\marginnote{The book uses the last line and uses it in 
some example. 
Perhaps that way of thinking is used later in the book (see page 10 of~\citep{seber2012linear}).}
\begin{equation}\label{eq:specialCaseWeird}
\begin{aligned}
E[{\bf X}^T {\bf A} {\bf X}] &= \sigma^2 tr({\bf A}) + \boldsymbol{\mu}_{\bf X}^T {\bf A}\boldsymbol{\mu}_{\bf X} \\ 
&= \sigma^2 (\text{sum of coefficients of } X_i^2) + ({\bf X}^T {\bf A} {\bf X})_{{\bf X} = \boldsymbol{\mu}_{\bf X}}
\end{aligned}
\end{equation}
\end{enumerate}


\begin{thm}
Let $X_1, X_2, \dots, X_n$ be independent random variables
with means $\mu_1, \mu_2, \dots, \mu_n$ and common variance
$\sigma_2$, common third and fourth moments around their means
$\sigma_3$ and $\sigma_4$ (i.e. $\sigma_r = E[(X_i - \mu_i)^r]$).
If ${\bf A} = {\bf A}^T\in \mathbb{R}^{n \times n}$
and ${\bf a}$ is a column vector of the diagonal elements of ${\bf A}$,
then
\[ var [{\bf X}^T{\bf A X}]  =  (\sigma_4 - 3\sigma_2^2) {\bf a}^T {\bf a} + 2\sigma_2^2 tr({\bf A}^2) + 4\sigma_2  \boldsymbol{\mu}^T{ \bf A}^2 \boldsymbol{\mu} + 4 \sigma_3 \boldsymbol{\mu}^T { \bf A a}  \]
\end{thm}

\subsection{Distributions of Functions of Random Variables}
\warn First thing we need to remember is the following: $E[f(X)] \neq f(E[X])$.
We need to learn \emph{distribution function technique}, \emph{the change-of-variable technique} and the \emph{moment-generating function technique}.

\begin{tcolorbox}
Summary of change of variable:

Let $X$ be a random variable with distribution function $f_X(x)$. 
Let $Y = g(X)$. Then

\[E[g(X)] = \int_{-\infty}^{\infty} y f_{Y}(y) dy =  \int_{-\infty}^{\infty} g(x) f_{X}(x) dx \]

In case monotonic $g: \Rto 1 1$ the density function is 
\[ f_Y(y) =f_X\left(g^{-1}(y) \right) \left\lvert \frac{d}{dy} g^{-1}(y) \right \rvert\]

In case of vectors and invertible function $H: \Rto n n$, $\mathbf{y} = H(\mathbf{x})$:
\[  g({\bf y}) = f(H^{-1} ({\bf y})) \left\lvert   det \left (\frac{\partial H^{-1}}{\partial{y}} \right )      \right \rvert \]
\end{tcolorbox}

\noindent \textbf{Distribution Function Technique}
\begin{enumerate}
\item Find cumulative distribution function $F_Y(y) = P(Y \le y)$.
\vspace{-0.5in}
\item Differentiate $F(y)$ to get $f(y)$.
\end{enumerate}

\begin{exm}
Let $f_X(x) = 3x^2$ on $[0, 1]$. 
Find the probability density function of $Y=X^2$.\\

\noindent {\color{red}{Solution.}} For $y \in [0, 1]$
\begin{equation}
\begin{aligned}
F_Y(y) &= p(Y \le y) \\
            &= p(X^2 \le y) \\
            &= p(X \le \sqrt{y}) \\
            &= F_X(\sqrt{y}) \\
            &= \int_{0}^{\sqrt{y}} 3t^2 dt \\
            &=  y^{3/2} \\ 
\end{aligned}
\end{equation} 
So, $f_Y(y) = F_Y'(y) = \frac{3}{2} y^{1/2}$.
\end{exm}

\iffalse
\begin{exm}
Let $f_X(x) = 3(1-x)^2$ on $[0, 1]$. 
Find the probability density function of $Y=(1-X)^3$.\\

\noindent {\color{red}{Solution.}} 
\begin{equation}
\begin{aligned}
F_Y(y) &= p(Y \le y) \\
            &= p[(1-X)^3 \le y] \\
            &= p(X >  1 - y^{1/3}) \\
            &= 1 - p(X \le  1 - y^{1/3})  \\
            &= 1 - \int_{0}^{1 - y^{1/3}}  3(1-t)^2 dt \\
            &= 1 - \left[ - (1-t)^3 \right]_0^{1 - y^{1/3}} \\ 
            &= 1 + \left[ (1 - 1 + y^{1/3})^3 - 1  \right]\\ 
            &= y \\ 
\end{aligned}
\end{equation} 
So, $f_Y(y) = F'_Y(y) = 1$.
\end{exm}
\fi 
\noindent \textbf{Change-of-Variable Technique}



