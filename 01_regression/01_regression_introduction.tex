
\section{Introduction}
\label{sec:Regression_Introduction}
In this section we look at the task
of prediction an output given some inputs.
Suppose a random variable $Y$ fluctuates 
about an unknown value $f$. In other words,
$Y = f + \varepsilon$. The $f$ is considered to be
the true response and $Y$ is observed response
where $\varepsilon$ could be the natural fluctuation
or errors in measurements; things that are
out of our control one way or the other.
We believe the values should lie on $f$.
So, we try to find $f$ through observed $y$ values.
Further assume the response variable $Y$
depends on independent variables
$X_1, X_2, \dots, X_p$.\sidenote{There might be other
factors that we are not aware of or have little effect on
modeling performance that we prefer to ignore them.
Those effects are all shoved into $\varepsilon$!}
Linear regression model is of the following form
\begin{equation}
f(x_1, \dots, x_p) = \beta_0 + \beta_1x_1 + \dots + \beta_p x_p
\end{equation}
\noindent which is \emph{\bf linear in parameters} $\beta_j$s.
For the  \emph{\bf linear model} the $x_i$s can
be function of other variables;
$f(x_1, \dots, x_p) = \beta_0 + \beta_1g(x_1) + \dots + \beta_p g(x_p)$
where $g(x)$ could be $x^2, sin(x), wz$ for some $w$ and $z$.
\marginnote{The notation and terminology in statistics sucks. If  
the $g(x_i)$'s are polynomials then the model is called ``polynomial regression''
even though it is also linear regression in the sense defined just here; in
the sense that the model is linear in the parameter $\beta_j$'s!!!}
Please note the variables $x_1, \dots, x_p$ are in fact known constants
present in the training set!
