\section{Linear Regression Exercises and Holes to Fill}
\label{sec:holesLinearRegression}

In this chapter we put the exercises to do
in order to fill the holes in my knowledge.

\begin{exc}
``Effective'' number of parameters in $k$-NN is $N/k$ (and is generally bigger than $p$, where $p$ is the number of variables).
\end{exc}
Intuition: note that if the neighborhoods were non-overlapping,
there would be $N/k$ neighborhoods and we would fit one parameter
(a mean) in each neighborhood.


\begin{exc}\label{ex:LRcond}
Let $Pr(X,Y)$ be the joint probability of input X and scalar output Y.
Define the squared error loss by $L(Y, f(X)) = (Y - f(X))^2$. This leads to
a criterion for choosing $f$,
\begin{equation}
EPE(f) = E[(Y-f(X))^2] = \int (Y-f(X))^2 Pr(dx, dy)
\end{equation}
By conditioning on $X$ we get:
\begin{equation}
EPE(f) = E_X E_{Y|X} E[(Y-f(X))^2 | X]
\end{equation}
and it suffices to minimize EPE pointwise:
\begin{equation}
f(x) = argmin_cE_{Y|X}[(Y-c)^2 | X=x]
\end{equation}
which gives the solution:\sidenote{The solution is conditional mean at any point $x$, if best is measured by average square error. NN methods attempt achieve this directly.}
\begin{equation}
f(x) = E[Y | X=x]
\end{equation}
\end{exc}

\begin{exc}\label{ex:varCovarfromTrain}
If $\mathbf{X}$ is full-rank then $\mathbf{X}^T\mathbf{X}$ is positive definite,
which means it is invertible. Then the solution for minimizing
$RSS(\theta)$ is 
\begin{equation}\label{eq:analySol}
\hat \beta = (\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}
Derive the variance-covariance of $\hat \beta$ from~\cref{eq:analySol}
which is
\begin{equation}
Var(\hat \beta) = (\mathbf{X}^T \mathbf{X})^{-1}\sigma^2
\end{equation}
Moreover, one estimates $\sigma$ by
\begin{equation}
\hat \sigma^2 = \frac{1}{N-p-1}\sum_{i=1}^N(y_i - \hat y_i)^2.
\end{equation}
Find out what it means to say $N-p-1$ makes the estimation unbiased.
\end{exc}


\begin{exc}
Derive Eq. (3.10) of the book from Eq. (3.9) in the book.
\end{exc}