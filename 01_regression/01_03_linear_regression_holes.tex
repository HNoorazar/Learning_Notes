\section{Linear Regression Exercises and Holes to Fill}
\label{sec:holesLinearRegression}

In this chapter we put the exercises to do
in order to fill the holes in my knowledge.

\begin{exc}
``Effective'' number of parameters in $k$-NN is $N/k$ (and is generally bigger than $p$, where $p$ is the number of variables).
\end{exc}
Intuition: note that if the neighborhoods were non-overlapping,
there would be $N/k$ neighborhoods and we would fit one parameter
(a mean) in each neighborhood.


\begin{exc}\label{ex:LRcond}
Let $Pr(X,Y)$ be the joint probability of input X and scalar output Y.
Define the squared error loss by $L(Y, f(X)) = (Y - f(X))^2$. This leads to
a criterion for choosing $f$,
\begin{equation}
EPE(f) = E[(Y-f(X))^2] = \int (Y-f(X))^2 Pr(dx, dy)
\end{equation}
By conditioning on $X$ we get:
\begin{equation}
EPE(f) = E_X E_{Y|X} E[(Y-f(X))^2 | X]
\end{equation}
and it suffices to minimize EPE pointwise:
\begin{equation}
f(x) = argmin_cE_{Y|X}[(Y-c)^2 | X=x]
\end{equation}
which gives the solution:\marginnote{The solution is conditional mean at any point $x$, if best is measured by average square error. NN methods attempt achieve this directly.}
\begin{equation}
f(x) = E[Y | X=x]
\end{equation}
\end{exc}

\subsection{Curse of High Dimensionality}
\label{sec:CurseofHighDimensionality}
Consider the nearest-neighbor procedure for inputs 
uniformly distributed in a p-dimensional unit hypercube, 
sas in Figure 2.6. Suppose we send out a hypercubical 
neighborhood about a target point to capture a fraction 
of the observations. Since this corresponds to a 
fraction $r$ of the unit volume the expected edge 
length will be $e_p(r) = r^{1/P}$. In ten dimensions 
$e_{10}(0.01)= 0.63$ and $e_{10}(0.1) = 0.80$, 
while the entire range for each input is only 1.0. 
So to capture 1\% or 10\%
of the data to form a local average, 
we must cover 63\% or 80\% of the range of each input variable. Such neighborhoods are no longer ``local.'' 
Reducing $r$ dramatically does not help much either, since
the fewer observations we average, the higher 
is the variance of our fit. 

Another consequence of 
the sparse sampling in high dimensions is 
that all sample points are close to an edge of the sample. 
Consider $N$ data point uniformly distributed in a 
$p-$dimensional unit ball centered at the origin. Suppose 
we consider a nearest-neighbor estimate at the origin. 
The median distance from the origin to the closest data point is given by the expression
\begin{equation}
d(p, N) = \left(1 - \frac{1}{2}^{1/N}\right)^{1/p}
\end{equation}

A more complicated expression exists for the mean 
distance to the closest point. For $N = 500$, $p = 10$, $d(p, N) \approx 0.52$, 
more than halfway to the boundary. Hence most data
 points are closer to the boundary of the sample 
 space than to any other data point. The reason 
 that this presents a problem is that prediction is 
 much more difficult near the edges of the training 
 sample. One must extrapolate from neighboring 
 sample points rather than interpolate between them.

Another manifestation of the curse is that the 
sampling density is proportional to $N^{1/P}$, where $p$
is the dimension of the input space and $N$ is 
the sample size. Thus, if $N_1 = 100$ represents 
a dense sample for a single input problem,
 then $N_{10} = 100^{10}$ is the sample size required 
 for the same sampling density 
 with 10 inputs. Thus in high dimensions 
 all feasible training samples sparsely populate the input space.

Let us construct another uniform example. 
Suppose we have 1000 training examples 
$x_i$ generated uniformly on $[-1, 1]^P$. 
Assume that the true relationship between X and Y is
\begin{equation}
Y = f(X) = e^{-8||X||^2},
\end{equation}
without any measurement error. We use 
the 1-NN rule to predict $y_0$ at 
the test-point $x_0=0$. Denote the training set by $\mathcal{T}$. 
We can compute the expected prediction error at $x_0$ for 
our procedure, averaging over all samples of size 1000.
Since the problem is deterministic, this is the mean squared error (MSE)
for estimating $f(0)$:
\begin{gather} % gather and aligned leads to having one label for eq.
\begin{aligned}
MSE(x_0) &= E_\mathcal{T}[(f(x_0) - \hat y_0)^2]\\
                 &= E_\mathcal{T}[(\hat y_0 - E_\mathcal{T}(\hat y_0))^2]
                       + E_\mathcal{T}[(E_\mathcal{T}(\hat y_0) - f(x_0))^2]\\
                 &= Var_\mathcal{T}(\hat y_0) + Bias^2(\hat y_0)
\end{aligned}
\end{gather}

Figure 2.7 illustrates the setup. We have broken 
down the MSE into two components that will 
become familiar as we proceed: variance and 
squared bias. Such a decomposition is always
possible and often useful, and is known as the
bias-variance decomposition. Unless the nearest 
neighbor is at 0, $\hat y_0$ will be smaller than $f(0)$ in this 
example, and so the average estimate will be 
biased downward. The variance is due to 
the sampling variance of the 1-nearest neighbor.
In low dimensions and with $N = 1000$, the 
nearest neighbor is very close to $0$, and so 
both the bias and variance are small. As the
dimension increases, the nearest neighbor 
tends to stray further from the target point, 
and both bias and variance are incurred. 
By $p = 10$, far more than 99\% of the samples 
the nearest neighbor is a distance greater than 
$0.5$ from the origin. Thus as $p$ increases, the 
estimate tends to be $0$ more often than not, and 
hence the MSE levels off at $1.0$, as does the 
bias, and the variance starts dropping (an artifact of this example).

Although this is a highly contrived example, 
similar phenomena occur more generally. The 
complexity of functions of many variables can 
grow exponentially with the dimension, and if 
we wish to be able to estimate such functions 
with the same accuracy as function in low dimensions, 
then we need the size of our training set to grow 
exponentially as well. In this
example, the function is a complex 
interaction of all $p$ variables involved. The 
dependence of the bias term on distance depends 
on the truth, and it need not always dominate with 
1-nearest neighbor. For example, if the function 
always involves only a few dimensions as in 
Figure 2.8, then the variance can dominate instead.

Suppose, on the other hand, that we know that
 the relationship between Y and X is linear,
\begin{equation}
Y = X^T\beta+ \epsilon,
\end{equation}
where $ \epsilon \sim N (0, \sigma^2)$ and we fit the model by 
least squares to the train ing data. For an arbitrary 
test point $x_0$, we have $y_0 = x_0^T \hat \beta$, which can be 
written as $\hat y_0 =  x_0^T \beta + \sum_{i=1}^N \ell_i(x_0)\eps_i$, where $ell_i(x_0)$ is the $i$th
element of ${\bf X} ({\bf X}^T {\bf X})^{-1}x_0$.  
Since under this model the least squares estimates are
unbiased, we find that
\begin{gather} % gather and aligned leads to having one label for eq.
\begin{aligned}
EPE(x_0) &= E_{y_0|x_0}E_\Tau[(y_0-\hat y _0)^2] \\
                &= Var(y_0|x_0) + E_\Tau[(\hat y_0 - E_\Tau[\hat y_0])^2] +
                      (E_\Tau[\hat y_0] - x_0^T \beta)^2 \\
                 &=  Var(y_0|x_0) + Var_\Tau(\hat y_0) + Bias^2(\hat y_0) \\
                 &= \sigma^2 + E_\Tau[x^T_0 ({\bf X }^T{\bf X })^{-1}x_0 \sigma^2] + 0^2
\end{aligned}
\end{gather} 
Here we have incurred an additional variance 
$\sigma^2$ in the prediction error, since our target is 
not deterministic. There is no bias, and the variance
depends on $x_0$. If $N$ is large and $\Tau$ were selected at 
random, and assuming $E[X] = 0$, then ${\bf X}^T {\bf X} \rightarrow N Cov(X)$ and \marginnote{They do not have any bracket for expected value. So, I may be off at times!}
\begin{gather} % gather and aligned leads to having one label for eq.
\begin{aligned}
E_{x_0}[ EPE(x_0)] &\sim E_{x_0} [x_0^T Cov(X)^{-1}x_0] \sigma^2/N+ \sigma^2 \\
                                &= \text{trace}[Cov(X)^{-1}Cov(x_0)] \sigma^2/N + \sigma^2 \\
                                &= \sigma^2 (p/N) + \sigma^2
\end{aligned}
\end{gather} 
Here we see that the expected EPE increases 
linearly as a function of $p$, with slope $\sigma^2/N$. 
If $N$ is large and/or $\sigma^2$ is small, this growth in variance 
is negligible ($0$ in the deterministic case). By 
imposing some heavy restrictions on the class of 
models being fitted, we have avoided the curse of 
dimensionality. \marginnote{Some of the technical details in 
(2.27) and (2.28) are derived in Exercise 2.5.}

