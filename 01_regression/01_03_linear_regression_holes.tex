\section{Linear Regression Exercises and Holes to Fill}
\label{sec:holesLinearRegression}

In this chapter we put the exercises to do
in order to fill the holes in my knowledge.

\begin{exc}
``Effective'' number of parameters in $k$-NN is $N/k$ (and is generally bigger than $p$, where $p$ is the number of variables).
\end{exc}
Intuition: note that if the neighborhoods were non-overlapping,
there would be $N/k$ neighborhoods and we would fit one parameter
(a mean) in each neighborhood.


\begin{exc}\label{ex:LRcond}
Let $Pr(X,Y)$ be the joint probability of input X and scalar output Y.
Define the squared error loss by $L(Y, f(X)) = (Y - f(X))^2$. This leads to
a criterion for choosing $f$,
\begin{equation}
EPE(f) = E[(Y-f(X))^2] = \int (Y-f(X))^2 Pr(dx, dy)
\end{equation}
By conditioning on $X$ we get:
\begin{equation}
EPE(f) = E_X E_{Y|X} E[(Y-f(X))^2 | X]
\end{equation}
and it suffices to minimize EPE pointwise:
\begin{equation}
f(x) = argmin_cE_{Y|X}[(Y-c)^2 | X=x]
\end{equation}
which gives the solution:\marginnote{The solution is conditional mean at any point $x$, if best is measured by average square error. NN methods attempt achieve this directly.}
\begin{equation}
f(x) = E[Y | X=x]
\end{equation}
\end{exc}

