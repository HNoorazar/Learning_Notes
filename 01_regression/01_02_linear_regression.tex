\section{Linear Regression}
\label{sec:Linear_Regression}

Let us assume we have $N$ observations/examples
where in each example there are $p$ measurements and one
response; i.e. there are $p$ independent variables.
We believe the response is a function of these variables; 
$y = f(x_1, x_2, \dots, x_p)$. However, in practice
this will not happen. There will be noise present and 
the $y$ values will be random variables. 
If our assumption ($y$ being function of
$x_i$s) is correct (i.e. we did not miss anything), 
$y$ will fluctuate about $f(\bm x)$ and we will model it via random 
variable concept and in practice we will have 
$y = f(x_1, x_2, \dots, x_p) + \varepsilon$ where $\varepsilon$
is out of our control. Please note I already abused the notation
and used $y$ to present both $f(\bm x)$ and $f(\bm x) + \varepsilon$\marginnote{So, we 
try to model $y$ is the same as saying we are modeling $f(\bm x)$.}.
Thus, even if $(x_1, x_2, \dots, x_p)$ is fixed, the $y$ values we
observe over time or at different experiment trials will be
different. $y$ is a random variable because the out-of-control-variable
$\varepsilon$ is.

Then, we have $y_i = \beta_0 + \beta_1x_{11} + \dots + \beta_p x_{1p} + \varepsilon_i$.
In the matrix form we have $\bm y = \bm X \bbeta+ {\bm \varepsilon}$
where $\bm X \in \mathbb{R}^{N \times (p+1)}$. Here we assume
$\bm X$ is full rank. Consider a scenario in which we
repeat each training example twice. Then, our model
thinks as if each training example is very confident.
Therefore, our testing tools will be undermined and the
power of tests will be reduced. Wrongfully we would think
the model is a good model.\marginnote{In some cases the matrix $\bm X$
will contain only zeros and ones and is referred 
to by design matrix. In these
cases $\bm X$ may not be full rank and we ignore them here.}

Let us carry on with the modeling. Let me add that
we try to model 
\begin{equation}\label{eq:overDeterminedSystem}
{\bm y} = {\bm X} \bbeta
\end{equation}
 as closely as we can
as opposed to ${\bm y} = {\bm X} \bbeta + {\bm \varepsilon}$ because
well clearly $\bm \varepsilon$ varies independent of 
$\bm x$ in the sense that even if $\bm x$ is fixed
we will have different values of $\bm \varepsilon$ and
consequently $y$ at different datasets/experiments.
Please note here in linear regression we are also assuming 
$\varepsilon$ is independent of $\bm x$ in the sense that
$\varepsilon$ is i.i.d. for all $\bm x$.

The system of linear equations  $\bm y = \bm X \bbeta$
is an overdetermined system; number of equations is more than
number of independent variables (features/regressors) i.e., 
$\bm X \in \mathbb{R}^{N \times p},~ N > p$.
The right-hand side is a vector in the space spanned by 
by the columns of $\bm X$. So, if $\bm y$ 
does not lie in this space,
then \cref{eq:overDeterminedSystem} 
does not have an exact solution!\sidenote{If we 
had 2 points, we could find a 1-dimensional 
line that passes through 
them. If we had 3 points, we could find a 
2-dimensional place that passes through them,
and so on, But in Eq.~\ref{eq:overDeterminedSystem} we
are expecting to find a plane of $p-$dimension such that
$N >>p$ points lie on it. In practice, those $N$ points will
not all lie on the $p-$dimensional space/surface!}
In other words, such an ``\emph{equality/equation}'' does not exist, 
even though we write it like an equation. Thus, we must
settle with an approximation. The chosen/popular approach
is to solve this system by least square method.
Since we assumed $\bm X$ is full rank there is a unique
\emph{solution} for the least square problem, 
i.e. minimizing squared 
sum of residuals\marginnote{In this notes mostly 
the second norm is used, so, I will use $\norm{.}$
for $\norm{.}_2$.}
$ r_i = y_i -  \bm x_i^T \bbeta$; 
\[ \norm{\bm y - \bm X {\bbeta}}_2 =\min_{\bbeta \in \mathbb{R}^p} \norm{\bm y - \bm X \bbeta}_2.\]

This problem is explained geometrically as well as algebraically
in the beautiful and smooth book by Watkins\cite{watkinsfund}.
The latter is done via $QR$ decomposition 
as well as SVD decomposition
which is one of the strongest tools in matrix 
computation toolbox. 

We know the shortest distance between a point and a line
is the one that connects the point perpendicularly to the line.
We generalize that idea to a higher dimension here.
Suppose $\bm y$ is the point and column space of 
$\bm X$, $\mathcal{R}(\bm X) = col(\bm X)$, is the line.
The shortest distance between the two is orthogonal
projection of $\bm y$ onto $\mathcal{R}(\bm X)$; $proj_{\mathcal{R}(\bm X)}(\bm y)$.
Then the distance between $\bm y$ and $\mathcal{R}(\bm X)$
is $\norm{\bm y - proj_{\mathcal{R}(\bm X)}(\bm y)}$.
We also know the vector $\bm X \bm \beta$ is in column space of $\bm X$.
So, the $\bm \beta$ for which 
$\norm{\bm y - \bm X \bm \beta}$ is minimized
is the one that gives us $proj_{\mathcal{R}(\bm X)}(\bm y)$.
Denote that solution by $\hat {\bm \beta_{LS}}$. Then, 
since $\bm y - \bm X \hat{\bm \beta_{LS}}$ is orthogonal to 
$\mathcal{R}(\bm X)$, it is orthogonal to all columns of $\bm X$;
$\langle \bm y - \bm X \hat{\bm \beta}_{LS}, \bm X_{k} \rangle = 0$
where $\bm X_k$ is the $k^{\text{th}}$ column of $\bm X$.
In matrix notation $\bm X^T (\bm y - \bm X \hat{\bm \beta_{LS}}) = 0$.
Therefore, 
\[
\bm X^T \bm X \hat{\bm \beta} = \bm X^T \bm y.
\]
Since we assumed $\bm X$ is full rank, $\bm X^T \bm X$
is positive definite and consequently invertible and we have
\[ \hat{\bbeta}_{LS} = (\bm X^T \bm X)^{-1} \bm X^T \bm y.\]
Now, $\bm y \approx \hat {\bm y}_{LS} = \bm X \hat {\bbeta}_{LS} = \bm X (\bm X^T \bm X)^{-1} \bm X^T \bm y = \bm H \bm y$
where $\hat {\bm y}_{LS}$ is orthogonal projection of $\bm y$
onto $\mathcal{R}(\bm X)$ and 
$\hat {\bm y}_{LS} = \bm X \hat {\bbeta}$
actually is an equality. The matrix $\bm H$ 
is called the hat matrix.
Even if $\bm X$ is not full rank, i.e. it is rank deficient, 
the least square problem given by overdetermined system 
${\bm y} = {\bm X \bm \beta}$
will have a solution/best approximation 
(in the sense of minimizing
the residuals) except that is not unique. There will
be infinitely many solutions~\citep{watkinsfund}.

\begin{thm}
Let $\bm X \in \mathbb{R}^{N \times p}$ be full rank and
$\bm H = \bm X (\bm X^T \bm X)^{-1} \bm X^T$. Then 
\begin{enumerate}[label=(\alph*)]
\item $\bm H$ and $\bm I_N - \bm H$ are symmetric 
and idempotent.
\item $rank(\bm I_N - \bm H) = tr(\bm I_N - \bm H) = N - p$.
\end{enumerate}
\end{thm}

\begin{exc}
Prove the followings.
\begin{enumerate}[label=(\alph*)]
\item  $\forall \bm X,~ \bm X^T \bm X$ is positive definite.
\item A positive definite matrix $\bm M$ is invertible.
\item If $\bm M$ is idempotent then $tr(\bm M) = rank(\bm M)$.
\end{enumerate}
\end {exc}

\section{Least Square Problem; A Numerical Analyst View}
\label{sec:Least-Square-Problem-A-Numerical-Analyst-View}
I would present a very short presentation of solving the
least square problem algebraically. I refer
the reader to~\citep{watkinsfund} for the details and even
for painting the geometric picture.

Before moving forward recall that if $\bm v \in \mathbb{R}^k$,
and $\bm Q \in \mathbb{R}^{k \times k}$ is an orthogonal matrix, 
i.e. $\bm Q^{-1} = \bm Q^T$, then 
$\norm{\bm Q \bm v} = \norm{\bm v}$.


Let $\bm X \bbeta = \bm y$ be an overdetermined system
where  $\bm X \in \mathbb{R}^{N \times p}, N > p, \bbeta \in  \mathbb{R}^p$, 
and  $\bm y \in \mathbb{R}^N$. Then, $\bm X$ 
has a $QR$ decomposition
with an orthogonal $\bm Q \in \mathbb{R}^{N \times N}$ and 
$\bm R \in \mathbb{R}^{N \times p}$:
\[ \bm X = \bm Q \bm R = \bm Q \begin{bmatrix} \hat{\bm R} \\ \bm 0 \end{bmatrix}\]
where $\hat {\bm R} \in  \mathbb{R}^{p \times p}$ is upper triangular.
This decomposition exist whether $\bm X$ is full rank or not.

Now consider the full rank case.
We can re-write the system as $\bm Q \bm R \bbeta = \bm y$ 
and consequently, $\bm R \bbeta = \bm Q^T \bm y$. 
Let $\bm c = \bm Q^T \bm y$, then $\bm R \bbeta = \bm c$.
Since $\bm Q$ is orthogonal the vectors 
$\bm r = \bm y - \bm X \bbeta$
and 
$\bm Q^T \bm r = \bm Q^T (\bm y - \bm X \bbeta) = \bm c - \bm R \bbeta$
are of the same size; i.e. minimizing $\norm{\bm r}$ is the same as
minimizing $\norm{\bm c - \bm R \bbeta}$.
Write $\bm c = \begin{bmatrix} \hat {\bm c} & \bm d \end{bmatrix}^T$,
then

\[ \bm c - \bm R \bbeta =  
\begin{bmatrix} \hat {\bm c} \\ \bm d  \end{bmatrix}  - \begin{bmatrix} \hat{\bm R} \\ \bm 0 \end{bmatrix} \bbeta = \begin{bmatrix} \hat {\bm c} - \hat{\bm R} \bbeta \\ \bm d  \end{bmatrix}.
\]
Therefore, 
\[\norm {\bm c - \bm R \bbeta}^2 = \norm{\hat {\bm c} - \hat{\bm R} \bbeta }^2 + \norm{\bm d}^2.\]
Obviously $\norm{\bm d}$ is independent of $\bbeta$.
Therefore, $\norm {\bm c - \bm R \bbeta}$ is minimized when
$\norm{\hat {\bm c} - \hat{\bm R} \bbeta } \ge 0$ is minimized.
Since we assumed $\bm X$ is full rank, $\hat{\bm R}$ is nonsingular
and the system $\hat{\bm R} \bbeta = \hat {\bm c} $ has a unique 
solution that can be found easily by back substitution.




\section{Properties of Least Square Estimates}
\label{sec:Properties-of-Least-Square-Estimates}
Assume $\bm X$ is full rank and $E[\varepsilon_i] = 0$, 
i.e. errors are unbiased, then least square 
estimates $\hat \bbeta$ of $\bbeta$
is unbiased:
\begin{equation*}
\begin{aligned}
E[\hat \bbeta_{LS}] &= E[\bm H \bm y]\\
                      &= \bm H E[ \bm y]\\
                      &= \bm H \bm X \bbeta \\
                      &= \hatM \bm X \bbeta \\
                      &= \bbeta.
\end{aligned}
\end{equation*}
Further, if we assume the errors are uncorrelated, i.e.
$cov[\varepsilon_i, \varepsilon_j] = 0$ if $i \neq j$ and 
$cov[\varepsilon_i, \varepsilon_i] =  var[\varepsilon_i] = \sigma^2$ 
(i.e. $var[\bm \varepsilon] = \sigma^2 {\bm I}_N$), then

\[var[\bm Y] = var[\bm Y - \bm X \bbeta ] = var[ \bm \varepsilon ]\]
Consequently, 
\begin{equation}
\begin{aligned}
var[\hat \bbeta_{LS}] &= var[\bm H \bm Y]\\
                         &= \bm H var[ \bm Y] \bm H^T\\
                         &= \bm H \sigma^2 \bm I_N \bm H^T\\
                         &= \sigma^2 \hatM (\hatM)^T \\
                         &= \sigma^2 \hatM \bm X (\bm X^T \bm X)^{-1} \\
                         &= \sigma^2 (\bm X^T \bm X)^{-1}.
\end{aligned}
\end{equation}
Why $\hat {\boldsymbol \beta}_{LS}$ is
chosen as an estimate of $\boldsymbol \beta$?
``For a reasonable class of estimates 
(linear unbiased estimates)  $\hat \beta_j$
has the smallest variance.''
The aforementioned statement does not depend
on the distribution of errors; i.e. normality or other
kind of distribution.
If $\varepsilon_i \sim N(0, 1)$,
i.e. ${\boldsymbol \varepsilon} \sim N({\bf 0}, \sigma^2 {\bf I})$, 
i.e. ${\bf Y} \sim N({\bf X}{\boldsymbol \beta}, \sigma^2 {\bf I})$,
then ${\bf v}^T \hat {\boldsymbol \beta}$ has minimum variance
among the entire class of unbiased estimates, not just
linear estimates.

\begin{tcolorbox}
The $\hat \beta_j$s are said to be best linear unbiased estimators
where \emph{best} mean having minimum variance.
Of course, here we are assuming the model, $E[y_i]=\beta_0+\beta_1x_i$,
is correct.

Please note that normality of errors have not come into play.
Confidence intervals and hypothesis testing is where 
normality of errors come into the picture.
\end{tcolorbox}


\begin{thm}
Let $\bm X$ be full rank or rank deficient.
Let $\hat {\bm Y}$ be the least square estimate of ${\bm Y}$
from the over determined system $\bm X \bbeta = \bm Y$ and 
$\tilde {\bm Y}$ be an estimation obtained by any linear 
unbiased estimator/method.
Then, $var[\bm v^T \hat {\bm Y}] <  var[\bm v^T \tilde {\bm Y}]$ 
and it is unique; of course this is implied by ``$<$'' as opposed to ``$\le$''.
\end{thm}

If $\varepsilon_i$s are i.i.d with $\varepsilon_i \sim N(0, \sigma^2)$, i.e., 
$\bm \varepsilon \sim N(\bm 0, \sigma^2 \bm I)$,
i.e., $\bm Y \sim N(\bm X \bbeta, \sigma^2 \bm I)$,
then $\bm v^T \hat \bbeta$ has minimum variance for entire class 
of estimates of $\bbeta$, not only linear estimations.\sidenote{
Seber uses $\hat \bbeta$ when $\bm X$ is full rank. So, I am not sure
whether this is true for rank deficient $\bm X.$}
Moreover, $\hat \beta_j$s will be the maximum likelihood estimations of $\beta_j$s
as well.

\section{Unbiased Estimation of $\sigma^2$}
\label{sec:Unbiased-Estimation-of-sigma-regression}
Let $var[\varepsilon_i] = \sigma^2$.

\begin{thm}
If $E[\bm Y] = \bm X \bbeta$ where $\bm X \in \mathbb{R}^{N \times p}$
matrix of rank $r \le p$, $var[\bm Y] = \sigma^2 \bm I$, and
$\hat{\bm Y}$ is the predictions obtained by least square approach
then
\[ S^2 = \frac{(\bm Y - \hat {\bm Y})^T (\bm Y - \hat {\bm Y})}{N -r} = \frac{RSS}{N-r} \]
is unbiased estimate of $\sigma^2$.
\end{thm}

\begin{thm}
Let $\bm Y \sim N_N(\bm X \bbeta, \sigma^2 \bm I)$.
Let $X \in \mathbb{R}^{N \times p}$ and $rank(\bm X) = p$.
Assume $\varepsilon_i \sim N(0, \sigma^2)$ so that
$\bm \varepsilon \sim N_N(\bm 0, \sigma^2 \bm I)$ and hence
$\bm Y \sim N_N(\bm X \bbeta, \sigma^2 \bm I)$. Then,
\begin{enumerate}[label=(\alph*)]
\item $\hat \bbeta_{LS} \sim N_p(\bbeta, \sigma^2 (\bm X^T\bm X)^{-1})$.

\item $\sigma^{-2} (\hat \bbeta_{LS} -\bbeta)^T \bm X^T \bm X (\hat \bbeta_{LS} -\bbeta) \sim \chi_p^2.$

\item $\hat \bbeta_{LS}$ is independent of $S^2$.

\item $RSS/\sigma^2 = (N-p) S^2 \sigma^{-2} \sim \chi_{N - p}^2$.
\end{enumerate}
\end{thm}

Another estimate of $\sigma^2$ is
$\hat \sigma^2 = RSS/N$ which is obtained
through the maximum likelihood route.

\section{Maximum Likelihood Estimation}
\label{sec:Maximum-Likelihood-Estimation-Regression}
\begin{deff}[Likelihood]
The function $\mathcal{L}(\Tau, \bm \theta)$ 
(also denoted by $\mathcal{L}(\Tau; \bm \theta)$ or 
simply $\mathcal{L} (\bm \theta)$), 
which is a function of 
parameter(s) $\bm \theta$, is known as the 
likelihood of the data $\Tau$.
\end{deff}
\noindent The likelihood function describes the joint probability 
of the observed data as a function of the parameters.
For each specific parameter value $\theta$  in the parameter space, the likelihood function $p(\Tau | \bm \theta )$ therefore assigns a probabilistic prediction to the observed data $\Tau$.\\

If we assume $\varepsilon_i \sim N(0, \sigma^2)$ just like
the section above, \cref{sec:Unbiased-Estimation-of-sigma-regression} 
then the likelihood function $\mathcal{L}(\bbeta, \sigma^2)$
for the full rank $\bm X$ is the probability density of $\bm Y$:
\[\mathcal{L}(\bbeta, \sigma^2) = (2\pi\sigma^2)^{-N/2} 
exp \left \{ -\frac{1}{2\sigma^2} \norm{\bm y - \bm X \bbeta}^2 \right \}\]
Let 
\[
\ell(\bbeta, \sigma^2) = log[\mathcal{L}(\bbeta, \sigma^2)] 
 = -\frac{N}{2}log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \norm{\bm y - \bm X \bbeta}^2,
 \]
then it follows that


\begin{equation}
\begin{cases} 
\frac{\partial \ell}{\partial \bbeta} = -\frac{1}{2\sigma^2} (-2\bm X^T \bm y + 2\bm X^T \bm X \bbeta) \\ \\
\frac{\partial \ell}{\partial \sigma^2} = -\frac{N}{2\sigma^2} + 
\frac{1}{2\sigma^4} \norm{\bm y - \bm X \bbeta}^2
\end{cases}
\end{equation}
Letting $\frac{\partial \ell}{\partial \bbeta} = 0$ gives us the
least square estimation of $\bbeta$, $\hat \bbeta_{LS}$,
which maximizes $\ell(\bbeta, \sigma^2)$ for \emph{any} $\sigma^2$;
$\mathcal{L}(\bbeta, \sigma^2) \le \mathcal{L}(\hat \bbeta_{LS}, \sigma^2)$
with equality if and only if $\bbeta = \hat \bbeta_{LS}$.
Let us maximize $\ell(\hat \bbeta_{LS}, \sigma^2)$.
Setting $\frac{\partial \ell}{\partial \sigma^2} = 0$ we get
$\hat \sigma^2 = \norm{\bm y - \bm X \hat \bbeta_{LS}}^2/N$.
Consequently,
\[\ell(\hat \bbeta_{LS}, \hat \sigma^2) - \ell(\hat \bbeta_{LS}, \sigma^2) = -\frac{N}{2}\left[log\left(\frac{\hat\sigma^2}{\sigma^2} \right) + 1 -\frac{\hat\sigma^2}{\sigma^2}\right] \ge 0. \]
For $x\ge 0$ we have $log(x) \le x-1$. Thus, for all $\sigma^2$
\[ \mathcal{L}(\bbeta, \sigma^2) \le \mathcal{L}(\hat \bbeta, \hat \sigma^2),\]
with equality \ifft $(\bbeta, \sigma^2) = (\hat \bbeta_{LS}, \hat \sigma^2)$.
Therefore, $(\hat \bbeta_{LS}, \hat \sigma^2)$ is the maximum
likelihood estimates of $(\bbeta, \sigma^2)$.
We will use the following fact later
\begin{equation}
\mathcal{L}(\hat \bbeta, \hat \sigma^2) = (2 \pi \hat \sigma^2)^{-N/2} e^{-N/2}.
\end{equation}






\vspace{1.5in}

If the \textit{best} is measured by average square error
then linear regression leads to the solution $f(x) = E[Y |X = x]$.\iffalse\marginnote{Please
note the $f(X)$ here is our estimate of $Y$. Perhaps a better
notation is $\hat f(X)$.... statisticians and their notations, again! }\fi
 The NN methods attempt fo fit the data by
finding the averages at each point in the input space.
Two approximations are happening here:
\begin{itemize}
\item expectation is approximated by averaging over sample data;
\item conditioning at point x is relaxed to conditioning on some region ``close'' to the target point (See Exc.~\ref{ex:LRcond}, especially the margin note). One can show that as $N,k \rightarrow \infty$
such that $k/N \rightarrow 0$, $\hat f(x) \rightarrow E[Y|X=x]$.
\end{itemize}

\hrule
\vspace{.1in}

While least squares is generally very convenient, it 
is not the only criterion used and in some cases would 
not make much sense. A more general principle for estimation is 
maximum likelihood estimation. Suppose we have a random 
sample $y_i, ~ i = 1, \dots , N$ from a density $Pr_{\theta}(y)$ 
indexed by some parameters $\theta$. The log-probability of the observed sample is
\begin{equation}
L(\theta) = \sum_{i=1}^N log(Pr_\theta(y_i))
\end{equation}
The principle of maximum likelihood assumes that the 
most reasonable values for $\theta$ are those for which 
the probability of the observed sample is largest. 
Least squares for the additive error model 
$Y = f_\theta(X) + \epsilon$, with $\epsilon \sim N(0,\sigma^2)$, is equivalent to 
maximum likelihood using the conditional likelihood
\begin{equation}
Pr(Y|X,\theta) = N(f_\theta(X), \sigma^2)
\end{equation}
So although the additional assumption of normality seems more restrictive,
the results are the same. The log-likelihood of the data is
\begin{equation}
L(\theta) = -\frac{N}{2} log(2\pi) - N log(\sigma) - \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - f_\theta(x_i))^2
\end{equation}
and the only term involving $\theta$ is the last, which is 
RSS($\theta$) up to a scalar negative multiplier.

\hrule
\vspace{.1in}
If the following does not make sense, read the
last paragraph on page 32 of~\citep{Hastie2001elements}.

Any function that passes through the points
$(x_i, y_i)$ will minimize the $RSS(\theta)$.
Hence, there are infinitely many solutions. So we have
to restrict the solutions to a smaller set.
Any restrictions imposed on f that lead to a unique 
solution to $RSS(\theta)$ do not really remove the ambiguity
caused by the multiplicity of solutions. There are infinitely 
many possible restrictions, each leading to a unique solution, 
so the ambiguity has simply been transferred to the choice of constraint.

\hrule
\vspace{.1in}
Typically we have a set of training data 
$(x_1, y_1) \dots (x_N , y_N)$ from which to estimate the 
parameters $\beta$. The most popular estimation
method is least squares, in which we pick the coefficients $\beta$ 
to minimize the residual sum of squares
\begin{gather} % gather and aligned leads to having one label for eq.
\begin{aligned}
RSS(\bbeta) &=  \sum_{i=1}^N (y_i - f(x_i))^2 \\ 
                   &=  \sum_{i=1}^N (y_i - \beta_0  - \sum_{j=1}^N x_{ij}\beta_j)^2\\ 
\end{aligned}
\end{gather} 
From a statistical point of view, this criterion is 
reasonable if the training observations $(x_i, y_i)$
represent independent random draws from their 
population. Even if the $x_i$'s were not drawn randomly, 
the criterion is still valid if the $y_i$'s are conditionally 
independent given the inputs $x_i$.



\section{Robust Regression}
\label{sec:Robust-Regression}
Robustness here refers to effect of noise
and corruption in the data on the fitted model.
For example, how bad outliers can affect the
fit or how much the noise in fraction of the data
can distort the fit.

``Robust statistics is statistics with good 
performance for data drawn from a wide 
range of probability distributions, especially 
for distributions that are not normal. Robust 
statistical methods have been developed for 
many common problems, such as estimating 
location, scale, and regression parameters. 
One motivation is to produce statistical 
methods that are not unduly affected by 
outliers. Another motivation is to provide 
methods with good performance when 
there are small departures from a parametric 
distribution. For example, robust methods 
work well for mixtures of two normal distributions 
with different standard-deviations; under this model, 
non-robust methods like a t-test work poorly.''\cite{robustWiki}

Least squares estimations of coefficients
are not good in presence of outliers. \marginnote{We have
seen this before, perhaps in the SVM book.}
This is because outliers will have a large error,
$e_i=r_i=Y_i - x_i^T\beta$, which will be even worse
when they are squared. Moreover, in least square we are
minimizing mean of errors, $\frac{1}{N}\sum r_i$.
Mean is not robust as well since it will be affected by outliers.
So, we can remedy these by using other metrics to measure
errors and also use another measure of \emph{location} instead of
mean; e.g. median or trimmed mean.

Using a different measure of size instead of squared error
leads to idea of M-estimation ($M$ for maximum likelihood) 
or idea of Rousseeuw and Yohai to minimize
a robust measure of the scale of residuals.

Using median or trimmed mean, an another remedy, 
leads to idea of least median of squares
and least trimmed squares, respectively.

In M-estimation method we start to take the route of 
maximum likelihood estimation which leads to estimating 
$\beta$ and $\sigma$. The techniques along this route leads
to choosing some functions that will produce robust results.
These functions dictate the measure of size and consequently 
(implicitly? Ex. 3.15 of ~\citep{seber2012linear}) location.
See~\citep{seber2012linear}, section 3.13, for more details.

As an alternative to M-estimators, we can replace 
the mean by a robust measure of location but retain 
the squared residuals as a measure of size; median or 
trimmed mean are two examples.

\section{Robustness Measures}
\label{sec:Robustness-Measures}

\section{Hypothesis Testing}
\label{sec:Hypothesis-Testing}
Suppose we have a general model
\[G: Y=\beta_0 + \beta_1 X_1 + \dots +\beta_{p-1}X_{p-1}+\varepsilon\]
and we want to test wether some of those coefficients are truly zero,
i.e. $H: \beta_r = \dots \beta_{p-1}=0$:
\[H: Y=\beta_0 + \beta_1 X_1 + \dots +\beta_{r-1}X_{r-1}+\varepsilon.\]

\noindent We need to look at 
\[
F=\frac{(RSS_H-RSS)/q}{RSS/N-p}
 = \frac{({\bf A \boldsymbol \beta - \bf c})^T [{\bf A} ({\bf X}^T{\bf X})^{-1}{\bf A}^T]^{-1} (\bf A \boldsymbol \beta - \bf c)}{qS^2}
 \sim F_{q, N-p}
\]
which has $F-$distribution with $q$ and $N-p$ degrees of freedom.
If $H$ is true then $F\approx 1$ and if $H$ is false then $F$ is large
and we reject the hypothesis.

In the second part of Eq. above we have $q$ and $\bf c$.
The reason is that the $H$ above can be generalized
and be written as $\bf A \boldsymbol \beta = \bf c$,
with $\bf A \in \mathbb{R}^{q \times {p-1}}$ where its
rows are linearly independent and $\bf c$ can be a vector
of zeros or not.\\

\begin{exm}\label{exm:HypTestOneCoeffLinReg}
From Seber:

Given the general linear model
\[Y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_{p-1}x_{i,p-1}\]
we can obtain a test statistic for $H:\beta_j=c$, where $j > 0$.
We first need the following partition:
\[ 
({\bf X}^T{\bf X})^{-1} = 
\begin{bmatrix}
l & {\bf m}^T\\
{\bf m} & {\bf D}
\end{bmatrix}
\]
where $l \in \mathbb{R}$.
Now $H$ is of the form ${\bf a}^T{\boldsymbol \beta}={\bf c}$, 
where ${\bf a}^T$ is the row vector with unity in 
the $j+1$ position and zeros elsewhere.
Therefore, using the general matrix theory, 
${\bf a}^T ({\bf X}^T{\bf X})^{-1} {\bf a} =d_{jj}$ 
(the jth diagonal element of $\bf D$), 
${\bf a}^T{\hat {\boldsymbol \beta}} - {\bf c}={\hat {\boldsymbol \beta}}_j - {\bf c}$, 
and the F-statistic is
\[F = \frac{({\hat {\boldsymbol \beta}}_j-c)^2}{S^2d_{jj}},\]
which has the $F_{1, N-p}$ distribution when $H$ is true. 
\end {exm}



In case of straight line we have 
$Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ and
we want to check $H:\beta_1 = c$.
We can either take derivative of 
$\boldsymbol{\varepsilon}^T\boldsymbol {\varepsilon}$
and set it to zero or compute ${\bf X}^T{\bf X}$ and so on
to see

\begin{gather} % gather and aligned leads to having one label for eq.
\begin{aligned}
\hat \beta_0 &= \overline Y - \hat \beta_1 \overline x \\ 
\hat \beta_1 &=  \frac{\sum Y_i (x_i-\overline x)}{\sum (x_i-\overline x)^2}= \frac{\sum (Y_i-\overline Y)(x_i -\overline x)}{\sum (x_i-\overline x)^2}\\ 
\hat Y_i &= \hat \beta_0 + \hat \beta_1 x_i = \overline Y + \hat \beta_1(x_i - \overline x).
\end{aligned}
\end{gather} 
\noindent From example~\ref{exm:HypTestOneCoeffLinReg}
we see
\[F = \frac{(\hat \beta_1 - c)^2}{S^2d_{11}} 
= \frac{(\hat \beta_1 - c)^2}{S^2/\sum (x_i - \overline x)^2},\]
where ($p=2$), $(N-2)S^2=\sum(Y_i - \overline Y)^2 - \sum(\hat Y_i - \overline Y)^2$.
With a little bit of work (See Seber, page 108)
we get 
\[
r^2 = \frac{\sum(\hat Y_i - \overline Y)^2}{\sum(Y_i - \overline Y)^2} 
= \frac{[\sum (Y_i - \overline Y)(x_i - \overline x)]^2}{\sum (Y_i - \overline Y)^2 \sum (x_i - \overline x)^2}
\]
which is the square of the sample 
correlation between $Y$ and $x$.
Also, $r$ is a measure of the degree of linearity 
between $Y$ and $x$ since
\[RSS = (1-r^2)\sum(Y_i - \overline Y)^2\]
so that the larger the value of $r^2$, the
smaller RSS and the better the fit of 
the estimated regression line to the observations.
Let us mention the following again

``Although $1 - r^2$ is a useful measure of fit, 
the correlation $r$ itself is of doubtful use in 
making inferences. Tukey [1954] makes the 
provocative but not unreasonable statement that 
correlation coefficients are justified in two 
and only two circumstances, when they 
are regression coefficients, 
or when the measurement of one or both 
variables on a determinate scale is hopeless.''\\

Generalization of $r^2$ for straight line leads to
$R^2$ (a.k.a. coefficient of demonstration):
\[R = \frac{\sum(Y_i - \overline Y)(\hat Y_i - \overline {\hat  Y})}{ \{ \sum(Y_i - \overline Y)^2 \sum(\hat Y_i - \overline {\hat  Y})^2\}^2}\]

and it can be shown:

\[R^2 = \frac{\sum (\hat Y_i - \overline Y)^2}{\sum (Y_i - \overline Y)^2} = 1 - \frac{RSS}{\sum (Y_i - \overline Y)^2}\]



