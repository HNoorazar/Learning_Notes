\section{Performance Metric}
\label{chap:Performance_Metric}

Consider an imbalanced dataset with 90\%
of the data from class $A$ and 10\% from class $B$.
Predicting every data point as class $A$ will result in a model
with 90\% accuracy. Accuracy is not a good performance metric.
Moreover, it assumes a false positive and a false negative 
cost the same. Labeling a sick person as a healthy person is a bad
mistake. However, labeling a healthy person as an ill person
is not as bad as it will become clear and corrected in the next
steps by examinations. True positive rate, false positive rate, true negative rate,
and false negative rate have the advantage of being independent 
of class costs and prior probabilities.

Some classifiers (e.g. logistic regression or some Neural Networks), 
yield a score that represents the degree to
which an example is a member of a class. 
Such ranking can be used to produce several classifiers, 
by varying the threshold of an example pertaining to a class. 
Each threshold value produces a different point in the ROC space. These
points are linked by tracing straight lines through two consecutive 
points to produce a ROC curve. 
ROC curves are consistent for a given problem, 
even if the distribution of positive and negative examples is highly skewed.
\begin{marginfigure}[-1\baselineskip]  % Vertical offset of 1 line,
  \centering
  %\captionsetup{justification=centering}
  \includegraphics[width=\textwidth]{00_figures/ROC_curve}
  \caption{An example of ROC curve.}
  \label{fig:ROCCurve}
\end{marginfigure}


ROC and AUC are insensitive to whether your predicted probabilities
are properly calibrated to actually represent the probability of
class memberships. i.e., the ROC/AUC would be identical even if
your predicted probabilities ranged from 0.9 to 1 instead of 0 to 1,
as long as the ordering of observations by predicted probability remained
the same. All AUC cares about is how well your classifier separated 
the two classes and it is only sensitive to rank ordering. 
AUC can be thought of as representing the probability that a classifier rank a random chosen positive observation higher than a randomly chosen negative 
observation.
So, it is useful for highly unbalanced/imbalanced classes.


