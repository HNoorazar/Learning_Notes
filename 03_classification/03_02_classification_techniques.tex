\section{Classification Techniques}
\label{chap:Classification_Techniques}

Linear classification techniques are those
whose decision boundaries are linear. Some methods
such as perceptron and optimally separating hyperplanes
attempt to find such boundaries directly. Some other models
such as linear regression, logistic regression, and linear discriminant analysis
do this implicitly. All is needed is to have a monotone transformation of
the $\delta_k(x)$--the discriminant functions for each class-- or the 
posterior probability --$P(G=k|X=x)$-- be linear.


\section{Linear Discriminant Analysis}
\label{sec:LinearDiscriminantAnalysis}
For optimal classification we need to know
the class posteriors; $P(G=k|X=x)$. 
Let $f_k(x)$ be the class-conditional density of
$X$ in class $G=k$, and let $\pi_k$ be the
prior probability of class $k$ with $\sum_{k=1}^K \pi_k=1$.
Then by Bayes theorem we have
\begin{equation}
P(G=k|X=x) = \frac{f_k(x)\pi_k}{\sum_{\ell=1}^K f_\ell(x)\pi_\ell}.
\end{equation}
Many models are based on class densities.
\begin{itemize}
\item Linear and quadratic discriminant analysis
\item Mixture of Gaussian densities that allows nonlinear decision boundaries
\end{itemize}
Suppose each class is modeled as a multivariate Gaussian
\begin{equation}
f_k(x) = \frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}_k|^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^T \mathbf{\Sigma_k^{-1}}(x-\mu_k)}
\end{equation}
Linear discriminant analysis (LDA) is the special case where we assume $\mathbf{\Sigma}_k=\mathbf{\Sigma}, \forall k$.
Comparing two classes $k$ and $\ell$ gives us the linear boundaries:
\begin{gather}\label{eq:LDALogOdd}
\begin{aligned}
log\left( \frac{P(G=k|X=x)}{P(G=\ell|X=x)}\right) &= 
log\left( \frac{f_k(x)}{f_\ell(x)}\right) + log\left(\frac{\pi_k}{\pi_\ell}\right)\\
&= log\left(\frac{\pi_k}{\pi_\ell}\right) - \frac{1}{2}\left(\mu_k+\mu_\ell\right)^T\mathbf{\Sigma}^{-1}\left(\mu_k-\mu_\ell\right) \\ &~~~~+ x^T\mathbf{\Sigma}^{-1}\left(\mu_k-\mu_\ell\right)
\end{aligned}
\end{gather}
The linear discriminant functions are given by
\begin{equation}
\delta_k = log\left(\pi_k\right) - \frac{1}{2}\mu_k^T\mathbf{\Sigma}^{-1}\mu_k+ x^T\mathbf{\Sigma}^{-1}\mu_k
\end{equation}
where we have to estimate the followings:
\begin{itemize}
\item $\hat \pi_k = N_k/N$
\item $\hat \mu_k = \sum_{g_i=k}x_i/N_k$
\item $\mathbf{\Sigma} = \sum_{k=1}^K \sum_{g_i=k}\left(x_i-\hat\mu_k\right)\left(x_i-\hat\mu_k\right)^T/(N-K)$
\end{itemize}
If the covariance matrices for each category are
not equal then the cancellations in~\cref{eq:LDALogOdd}
do not occur and we get quadratic discriminant analysis:
\begin{equation}
\delta_k = log\left(\pi_k\right) - \frac{1}{2}\left(x-\mu_k\right)^T\mathbf{\Sigma}_k^{-1}\left(x-\mu_k\right) - \frac{1}{2}log\left(|\mathbf{\Sigma}_k|\right)
\end{equation}

Both LDA and QDA perform well on an amazingly 
large and diverse set of classification tasks. For example, 
in the STATLOG project (Michie et al., 1994) LDA 
was among the top three classifiers for 7 of the 22 
datasets, QDA among the top three for four datasets, 
and one of the pair were in the top three for 10 datasets. 
Both techniques are widely used, and entire books 
are devoted to LDA. It seems that whatever exotic tools 
are the rage of the day, we should always have available 
these two simple tools. The question arises why LDA and 
QDA have such a good track record. The reason is not likely 
to be that the data are approximately Gaussian, and in 
addition for LDA that the covariances are approximately 
equal. More likely a reason is that the data can only 
support simple decision boundaries such as linear or 
quadratic, and the estimates provided via the Gaussian 
models are stable. This is a bias variance tradeoffâ€”we 
can put up with the bias of a linear decision boundary 
because it can be estimated with much lower variance
than more exotic alternatives. This argument is less 
believable for QDA, since it can have many parameters 
itself, although perhaps fewer than the non-parametric alternatives.
We will see generalization of LDA 
in~\cref{sec:Generalizing_Linear_Discriminant_Analysis}.\\

Some of the virtues of LDA are as follows:
\begin{itemize}
\item It is a simple prototype classifier. A new observation 
is classified to the class with closest centroid. 
A slight twist is that distance is measured in the 
Mahalanobis metric, using a pooled covariance estimate.

\item LDA is the estimated Bayes classifier if the 
observations are multi- variate Gaussian in each 
class, with a common covariance matrix. Since this 
assumption is unlikely to be true, this might not seem 
to be much of a virtue.
\item The decision boundaries created by LDA are 
linear, leading to decision rules that are simple to 
describe and implement.

\item LDA provides natural low-dimensional views of the data.

\item Often LDA produces the best classification 
results, because of its simplicity and low variance. 
LDA was among the top three classifiers for 7 of the 22 
datasets studied in the STATLOG 
project\cite{michie1994machine}.
\end{itemize}
Unfortunately the simplicity of LDA causes it to fail in 
a number of situations as well:
\begin{itemize}
\item Often linear decision boundaries do not 
adequately separate the classes. When $N$ is large, 
it is possible to estimate more complex decision 
boundaries. Quadratic discriminant analysis (QDA) 
is often useful here, and allows for quadratic 
decision boundaries. More generally we would 
like to be able to model irregular decision boundaries.

\item The aforementioned shortcoming of LDA 
can often be paraphrased by saying that a 
single prototype per class is insufficient. LDA 
uses a single prototype (class centroid) plus a 
common covariance matrix to describe the 
spread of the data in each class. In many 
situations, several prototypes are more appropriate.

\item At the other end of the spectrum, we 
may have way too many (correlated) predictors, 
for example, in the case of digitized analogue 
signals and images. In this case LDA uses too 
many parameters, which are estimated with high 
variance, and its performance suffers. In cases such 
as this we need to restrict or regularize LDA even further.
\end{itemize}




