\chapter{ML System Design}
\label{chap:ML_System_Design}

\begin{description}
\item [Problem statement]
It’s important to state the correct problems. It is the candidates job to understand the intention of the design and why it is being optimized. It’s important to make the right assumptions and discuss them explicitly with interviewers. For example, in a LinkedIn feed design interview, the interviewer might ask broad questions:

\begin{center}
Design LinkedIn Feed Ranking.
\end{center}

Asking questions is crucial to filling in any gaps and agreeing on goals. The candidate should begin by asking follow-up questions to clarify the problem statement. For example:
\begin{itemize}
\item Is the output of the feed in chronological order?
\item How do we want to balance feeds versus sponsored ads, etc.?
\end{itemize}
If we are clear on the problem statement of designing a Feed Ranking system, we can then start talking about relevant metrics like user agreements.

\item [Identify metrics]
During the development phase, we need to quickly test model performance using offline metrics. 
You can start with the popular metrics like logloss and AUC for binary classification, or RMSE and MAPE for forecast.

\item [Identify]  requirements
\begin{enumerate}
\item Training requirements
\begin{itemize}
\item There are many components required to train a model from end to end. These components include the data collection, feature engineering, feature selection, and loss function. For example, if we want to design a YouTube video recommendations model, it’s natural that the user doesn’t watch a lot of recommended videos. Because of this, we have a lot of negative examples. The question is asked:

\begin{center}
How do we train models to handle an imbalance class?
\end{center}

Once we deploy models in production, we will have feedback in real time.

\begin{center}
How do we monitor and make sure models don’t go stale?
\end{center}
\end{itemize}

\item Inference requirements
Once models are deployed, we want to run inference with low latency (<100ms) and scale our system to serve millions of users.

\begin{center}
How do we design inference components to provide high availability and low latency?
\end{center}
\end{enumerate}


\item [Train and evaluate model]~~
\begin{itemize}
\item There are usually three components: feature engineering, feature selection, and models. We will use all the modern techniques for each component.

\item For example, in Rental Search Ranking, we will discuss if we should use ListingID as embedding features. In Estimate Food Delivery Time, we will discuss how to handle the latitude and longitude features efficiently.
\end{itemize}

\item [Design high level system]
In this stage, we need to think about the system components and how data flows through each of them. The goal of this section is to identify a minimal, viable design to demonstrate a working system. We need to explain why we decided to have these components and what their roles are.

\begin{itemize}
\item For example, when designing Video Recommendation systems, we would need two separate components: the Video Candidate Generation Service and the Ranking Model Service.
\end{itemize}

\item [Scale the design]
In this stage, it’s crucial to understand system bottlenecks and how to address these bottlenecks. You can start by identifying:

\begin{itemize}
\item Which components are likely to be overloaded?
\item How can we scale the overloaded components?
\item Is the system good enough to serve millions of users?
\item How we would handle some components becoming unavailable, etc.
\item You can also learn more about how companies scale there design here.
\end{itemize}

\end{description}

In business there are metrics to evaluate performance of the model.
Of course, since it is business, they care about money.
Thus, the metric they want to hear is how much revenue is increased.
These metrics may not be convex, smooth, etc. So, they may not be
used during training phase. 
But, hopefully, the metrics used during the training phase
are good surrogate for the business-metric. 
Let's see what is out there.
At this point, Nov. 2, 2025, these are from edicative.io course; \emph{Machine Learning System Design}.
Hopefully, I will remember to cite everything properly.


\section{Prediction}
They call it inference.

\begin{description}
\item [Imbalance workload] Use load balancers to send clients' request
to different servers. Load balancers can follow different logics
\begin{enumerate}
\item Work load

\item  Round Robin: Sends each request to a new server in a rounding fashion. 
Server 1, then 2, then 3, so on

\item Request parameter: considers some parameters into account:
geographical location: send to a server near user, or device type: send to a server 
that is optimized for that device or has a model that is trained for users with that device.
Or of a server needs to remember sth; e.g. shopping cart.
\end{enumerate}

\item [Non-stationary problem]
In an online setting, data is always changing. Therefore, the data distribution shift is common. So, keeping the models fresh is crucial to achieving sustained performance. Based on how frequently the model performance degrades, we can then decide how often models need to update/retrain. One common algorithm that can be used is the Bayesian Logistic Regression.

\item [Exploration vs. exploitation:] {\bf  \color{red}Thompson Sampling}
In an Ad Click prediction use case, it’s beneficial to allow some exploration when recommending new ads. However, if there are too few ad conversions, it can reduce company revenue. This is a well-known exploration-exploitation trade-off. One common technique is Thompson Sampling where at a time, we need to decide which action to take based on the reward.

\end{description}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ML System Design (Educative.io)}
\label{sec:ML_System_Design_Educative}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{00_figures/systemDesignWorkFlow}
\caption{The 6 basic steps to approach Machine Learning System Design}
\label{fig:MLSystemDesignWorkFlow}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item Is the output of the feed in chronological order?
\item How do we want to balance feeds versus sponsored ads, etc.?
\end{itemize}


\begin{description}
\item [Offline Metrics]
logloss and AUC for binary classification, or RMSE and MAPE for forecast.

In Click Through Rate (CTR) prediction, Facebook uses Normalized Cross Entropy loss (a.k.a. logloss) to make the loss less sensitive to the background conversion rate. 

\item [Online metrics]
Lift in revenue or click through rate, to evaluate how well the model recommends relevant content to users. Consequently, we evaluate the impact on business metrics. If the observed revenue-related metrics show consistent improvement, then it is safe to gradually expose the model to a larger percentage of real traffic

\item [Exploration vs. exploitation] Thompson Sampling

\item [A/B testing]
\href{https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/}{ Sage Maker enables A/B testing} and
\href{https://www.linkedin.com/blog/engineering/archive/top-five-lessons-from-running-a-b-tests-on-the-world-s-largest-p}{LinkedIn A/B testing.}

\end{description}
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Video Recommendation}
\noindent {\bf \color{blue}  Offline Metrics} 
\begin{itemize}
\item Precision \& recall: How accurate and complete are our recommendations?
\item Ranking loss: Measures how well the system ranks relevant videos higher.
\item Log loss: Captures the model's confidence in its predictions.

\item Recall = how many relevant videos were retrieved.
Precision = how many of the shown videos were actually relevant.

Precision/Recall are simpler binary relevance metrics, good when the focus is on relevance detection rather than fine-grained ranking. That's why nDCG is not used here. In airBnB ranking is very important. In youtube video, relevance detection is more important than fine-grained ranking.
\end{itemize}

\noindent {\bf \color{blue}  Online Metrics} 
\begin{itemize}
\item Click-through rate (CTR): Do users click the videos we recommend?
\item Watch time: How long do they stay engaged with the content?
\item Conversion rate: Do they take desired actions like subscribing, liking, or sharing?
\end{itemize}

\begin{description}
\item [The candidate generation]  can be done by \hl{Matrix Factorization}. 
The purpose of candidate generation is to generate ``somewhat'' relevant content to users based on their watched history. The candidate list needs to be big enough to capture potential matches for the model to perform well with desired latency.
One solution is to use collaborative algorithms because the inference time is fast, and it can capture the similarity between user taste in the user-video space.

In practice, for large scale system (Facebook, Google), we don't use Collaborative Filtering and prefer low latency method to get candidate. One example is to leverage Inverted Index (commonly used in Lucene, Elastic Search). Another powerful technique can be found \hl{FAISS or Google ScaNN}.

\item [Ranking model] A fully connected neural network is simple yet powerful for representing non-linear relationships, and it can handle big data.
\end{description}
\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{00_figures/videoRecommender}
\caption{Video recommender design}
\label{fig:videoRecommender}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LinkedIn Feed Ranking}
\noindent {\bf \color{blue}  Offline Metrics:} normalized cross-entropy. AUC.


\noindent {\bf  \color{blue}  Online Metrics} 
Conversion rate (ratio of clicks with number of feeds).\vspace{-.2in}\sidenote{I  am not sure why they are referring to \emph{ratio of clicks with number of feeds}
as conversion rate. It seems that is CTR, not conversion rate.
Conversion rate: (CVR) measures how often a desired action happens after exposure or a click, depending on how it's defined in your system (Conversion: purchase, signup, install, add-to-cart, etc.).}

\noindent {\bf \color{blue} Feed Ranking Models} 
 Logistic Regression in Spark or Alternating Direction Method of Multipliers. We can also use deep learning in distributed settings. We can start with the fully connected layers with the Sigmoid activation function applied to the final layer.\\

\begin{figure}[ht]
\centering
\smallskip
\includegraphics[width=\textwidth]{00_figures/linkedInFeedRanking}
\vspace*{0.5in}
\caption{LinkedIn FeedRanking}
\label{fig:linkedInFeedRanking}
\end{figure}


\noindent {\bf  \color{blue} Evaluation} One approach is to split the data into training data and validation data. Another approach is to replay the evaluation to avoid biased offline evaluation. 
\sidenote{This is a language that used in recommender systems. This refers to replay-based (counterfactual) evaluation, also called policy replay or off-policy evaluation. {\bf Bias}: User saw items chosen by the old system $\rightarrow$ clicked some of them. You cannot fairly evaluate a new recommender on this data because:
You don’t know how users would have reacted to items not shown}
We use data until time $t$ for training the model. We use test data from time $t+1$
 and reorder their ranking based on our model during inference. If there is an accurate click prediction at the correct position, then we record a match. The total match will be considered as total clicks.
 
 
During evaluation we will also evaluate how big our training data set should be, and how frequently we should retrain the model, among many other hyperparameters.
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ad click prediction}
\noindent {\bf \color{blue} Offline Metrics}
Normalized Cross-Entropy (NCE): NCE is the predictive logloss divided by the cross-entropy of the
background CTR. This way NCE is insensitive to background CTR. This is the NCE formula
(This metric penalizes confident but wrong predictions—perfect for probabilistic classifiers.):
\[
\text{NCE} = \frac{}{}
\]


\noindent  {\bf \color{blue} Online Metrics}
Revenue Lift: Percentage of revenue changes over a period of time. Upon deployment, a new model is deployed on a small percentage of traffic. The key decision is to balance between percentage traffic and the duration of the A/B testing phase.

\begin{figure}[ht]
\centering
\includegraphics[width=.8\textwidth]{00_figures/adClickPredDesign}
\caption{ad Click Pred Design}
\label{fig:adClickPredDesign}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{AirBnB}
\noindent  {\bf \color{blue} Offline Metrics}
\begin{description}
\item [Normalized discounted Cumulative Gain: nDCG] is a standard metric in ranking problems where position matters. It gives higher weight to correct predictions near the top of the list—exactly what we want in search ranking.


\begin{tcolorbox}[colback=aliceblue!10!white,colframe=blue!70!black,title=\textbf{Why nDCG?}]
% olback=green!10!white,colframe=green!80!black
Users rarely scroll through all results. A relevant result at position 2 is more valuable than at position 10. 
It accounts for both relevance and position, unlike basic accuracy or AUC. 
It reflects user satisfaction better than simple classification metrics like precision or recall.
\end{tcolorbox}

\begin{align}
\text{DCG}_p &= \sum_{i=1}^{p} \frac{\text{rel}_i}{\log_2(i+1)} \\[6pt]
\text{nDCG}_p &= \frac{\text{DCG}_p}{\text{IDCG}_p},
\end{align}
$\text{rel}_i$= relevance score of the result at position $i$ and 
IDCG = ideal DCG, if all relevant results were perfectly ordered\sidenote{We need to expand on these. What are they?}

\end{description}


\noindent  {\bf \color{blue} Online Metrics}
conversion rate = number of bookings / number of search results\\


Use stratified sampling, class weighting, or \hl {focal loss} to avoid biasing toward the majority class.\\


\noindent  {\bf \color{blue} Cold-start challenge} New listings have no data. Use hybrid models or backfill with rule-based heuristics (e.g., prioritize listings with high-quality photos or competitive pricing).


\noindent {\bf Backfill}
\begin{description}
\item [For new items]: The backfill model typically leverages content-based features (e.g., item attributes, categories, keywords, descriptions, or images) to recommend them to users whose preferences align with these features. A common simple backfill strategy is to display the most popular or trending items in general to new users or alongside new items.

\item [For new users]: The backfill model might use demographic or contextual data (e.g., location, device type, time of day). Often, new users are shown general popular or top-rated items, or the system might explicitly ask them for their preferences during onboarding to quickly gather initial signals (active learning). 
\end{description}


\noindent {\bf Hybrid model}
 They strategically integrate methods like content-based filtering, collaborative filtering, popularity-based approaches, and auxiliary data to provide effective recommendations from the outset. 

\begin{description}
\item [For New Users] (User Cold-Start)
\begin{itemize}
\item Content-Based Filtering (CBF) Integration: The hybrid system can initially rely on a CBF approach, which uses user profile information (e.g., demographics, declared preferences during onboarding surveys) and item metadata (e.g., genre, keywords, descriptions) to make initial recommendations. This does not require past interaction data.

\item Popularity-Based Fallback: A common initial strategy is to recommend the most popular, trending, or top-rated items to new users until enough interaction data is collected to build a personalized profile.

\item Leveraging External Data: Hybrid systems can incorporate data from external sources, such as social networks, to infer a new user's interests and make initial suggestions.

\item Feature Combination: Combining explicit ratings and implicit feedback (like browsing history, adding items to a cart) into a single model helps build a more robust user representation faster, enabling a smoother transition to personalized recommendations. 
\end{itemize}

\item [For New Items] (Items Cold-Start)
\begin{itemize}
\item Content-Based Filtering (CBF): This is highly effective for new items as it uses their metadata (e.g., category, description, features) to match them with existing user preferences.

\item Feature Augmentation/Mapping: Side information about new items (e.g., price, brand, description) can be integrated into the model's feature space, allowing the system to generate a latent representation (embedding) for the new item without any interaction data.

\item Knowledge-Guided Retrieval: Utilizing a knowledge graph that connects new items to existing entities based on their attributes helps in retrieving relevant knowledge and making "zero-shot" recommendations. 
\end{itemize}

\end{description}


\begin{figure}[ht]
\centering
\includegraphics[width=.6\textwidth]{00_figures/airBnBDesign}
\caption{airBnB Design}
\label{fig:airBnBDesign}
\end{figure}


\begin{table}[h!]
\begin{fullwidth} % allows the table to extend across both columns
\hspace{-.2in}
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Platform} & \textbf{Main User Action} & \textbf{Model Output} & \textbf{Goal} & \textbf{Metric} \\
\hline
\textbf{Airbnb} & Search/browse top listings & Relevance score & Correct \textbf{ordering} & nDCG \\
\hline
\textbf{YouTube} & Watch recomm. videos & Relevance Y/N & Find \textbf{relevant} videos & Precision/Recall \\
\hline
\textbf{LinkedIn} & Scroll feed, engage & Prob. of engage. & Pred. \textbf{likelihood} accur. & Cross-Entropy \\
\hline
\end{tabular}
\end{fullwidth}
\end{table}


\newpage
\subsection{Food Delivery Time}

\noindent  {\bf \color{blue} Offline Metrics}  RMSE

\noindent  {\bf \color{blue} Online Metrics} RMSE and customer engagement. Conduct A/B testing to see 
model with overestimation is more successful or underestimation.

In the design we have
\begin{itemize}
\item Feature Store: Provides fast lookup for low latency. A feature store with any key-value storage with high availability like Amazon DynamoDB is a good choice.

\item Feature pipeline: Reads from Kafka, transforms, and aggregates near real-time statistics. Then, it stores them in feature storage.

\item Database: Delivery Order database stores historical Orders and Delivery. Data prep is a process to create training data from a database. We can store training data in cloud storage, for example, S3.

\item We have three services: Status Service, Notification Service, and Estimate Delivery Time service. The first two services handle real-time updates and the Estimate Delivery Time service uses our Machine Learning Model to estimate delivery time.

\item We have a scheduler that handles and coordinates retraining models multiple times per day. After training, we store the Model in Model Storage.
\end{itemize}


\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{00_figures/foodDeliveryDesign}
\caption{Food Delivery Time}
\label{fig:foodDeliveryDesign}
\end{figure}
