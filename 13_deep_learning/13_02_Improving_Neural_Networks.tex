\section{Improving Neural Networks: Hyper-parameter tuning, Regularization, and Optimization}\label{ImprovingNeuralNetworks}

\begin{description}
\item [Train, Validation, and Test Sets] When the dataset is small (e.g. less than 1000), 
one can divide the data into
training/validation/test set like so 70\%, 30\% (or 60\%, 20\%, 20\%).
However, in the big data era with large datasets (e.g. 1,000,000 data)
the percentage of validation/test set is smaller as percentage (e.g. 98\%, 1\%, 1\%).

\item [Data Distribution] Shuffle the data so that the (training,) validation 
and test sets come from the same distribution. 
\end{description}

To understand the bias and variance we look at the
training and validation errors (assuming human error or Bayes error is approximately 0\%):

\begin{table}[h!]
\begin{tabular}{|l|l|l|l|l|}
\hline
Test Error          & 1\%           & 15\%      & 15\%         & 0.5\% \\ \hline
Validation Error & 11\%          & 16\%      & 30\%    & 1\%   \\ \hline
                         & High Variance & High Bias & \begin{tabular}[c]{@{}l@{}}High Bias and \\ High Variance\end{tabular} & Happy \\ \hline
\end{tabular}
\caption{\label{tab:TrainValError} Train and Validation Error Rates.}
\end{table}

Training Error provides a sense of bias problem and how much
is the difference between validation error and training error provides
a sense of variance.

Below is a flowchart of a \textbf{basic} recipe for machine learning.
In the first step try to reduce the bias.
In deep learning and era of big data there is not a trade-off between
variance and bias. One can use a big network to reduce bias 
without necessarily hurting variance. In this case regularization
is helpful to take care of the variance. One can get  a large number of data
to reduce variance without hurting the bias.\\

\begin{tikzpicture}[node distance=2cm]
\node (HighBias) [startstop] {High Bias?};
\node (withBias) [io, right of=HighBias, xshift=1cm] {Yes};
\node (withBiasSolution) [process, right of=withBias, xshift=3cm] {Bigger Network,
Train Longer, (architecture)};

\node (noBias) [io, below of=HighBias] {No};
\node (HighVariance) [startstop, below of=noBias] {High Variance?};
\node (withVariance) [io, right of=HighVariance, xshift=1cm] {Yes};
\node (withVarSolution) [process, right of=withVariance, xshift=3cm] {More Data, Regularization, (architecture)};

\node (noVariance) [io, below of=HighVariance] {No};
\node (Done) [decision, below of=noVariance] {Done};

\draw [arrow] (HighBias) -- (noBias);
\draw [arrow] (HighBias) -- (withBias);
\draw [arrow] (withBias) -- (withBiasSolution);
\draw [arrow] (noBias) -- (HighVariance);
\draw [arrow] (HighVariance) -- (withVariance);
\draw [arrow] (HighVariance) -- (noVariance);
\draw [arrow] (withVariance) -- (withVarSolution);
\draw [arrow] (noVariance) -- (Done);

\end{tikzpicture}

\subsection{Regularization}\label{Regularization}
Having high variance (overfitting) can be taken care of by regularization
or obtaining more data which is not always possible. Below is a list of
regularization techniques.
\begin{enumerate}
\item L2 regularization
\item Dropout regularization
\item Obtaining more data (data augmentation; rotation, reflection, cropping)
\item Early stopping; plot train and validation errors.
\end{enumerate}


One of the ways of regularization is similar to L2-regularization (reffered to as
\emph{weight decay} here):
\begin{equation}
J(w^{[1]}, b^{[1]}, \dots, w^{[l]}, b^{[l]}) = 
\frac{1}{m} \sum_{i=1}^m \ell(\hat y^{(i)} - y^{(i)}) + 
 \frac{\lambda}{2m}\sum_{l=1}^{L} ||W^{[l]}||_F^2
\end{equation}

Then we have

\begin{equation}
dW^{[l]} = \frac{\partial J}{\partial W^{[l]}} = (from~backprop) + \frac{\lambda}{m} W^{[l]}
\end{equation}

and the updates will be as follows:
\begin{gather} % gather and aligned leads to having one label for eq.
\begin{aligned} 
W^{[l]} &= W^{[l]} - \alpha dW^{[l]} \\
           &= W^{[l]} - \alpha \left[(from~backprop) + \frac{\lambda}{m} W^{[l]} \right] \\ 
           &= W^{[l]} - \alpha \frac{\lambda}{m} W^{[l]} - \alpha (from~backprop) 
\end{aligned} 
\end{gather} %
The first two terms in above equation is referred to as
\emph{weight decay}.\\

In \textbf{drop-out regularization}, (for each training example) at every training step, 
we train the model on a smaller network where the smaller
network is obtained by tossing neurons randomly. The
structure of the network will be different for each training example.
For test examples there will not be any drop out.

One can have different probability of dropping out
for each layer. Specifically, the layers with large number
of weights can enjoy a higher drop out rate/probability.
In practice probability of dropping out is close to 0.

In computer vision, where there are not a lot of data, the
overfitting is a problem and consequently drop out method
is often used.\\

\textbf{Early stopping} is a technique for regularization. However,
it is interfering with the task of optimizing the cost function.
Let me elaborate. In ML it is a good practice to think and work
on a given task (e.g. minimizing the cost function for which we have
a set of tools, or, reducing the overfitting) one at a time. However,
early stopping which is used for avoiding overfitting
couples the problem at hand with the task of minimizing the cost function.

\subsection{Normalizing the Inputs}\label{NormalizingtheInputs}
Normalizing the inputs helps with the training process.
It makes the learning process faster (think about steps of gradient descent in a very
elliptical level sets of cost function). 

\subsection{Vanishing/Exploding Gradients}\label{Vanishing/ExplodingGradients}
A partial solution for avoiding the problem of vanishing/exploding
the weights is to be careful with the initialization of the weights.
Let the number of ingoing features of a neuron on layer $l$ be $n^{[l-1]}$.
For that layer we have $z = w_1 x_1 + \dots + w_n{^{[l-1]}} x_n{^{[l-1]}}$.
Consequently, one can do the following for initialization of
the weights:
\[W^{[l]} = np.random.randn(proper~shape) * np.sqrt(1/n^{[l-1]})\]
If ReLu is the activation function when one can/is advised to replace 
$np.sqrt(1/n^{[l-1]})$ with $np.sqrt(2/n^{[l-1]})$.

\subsection{Mini-batch Gradient Descent}
Rather than using the whole training data 
$\mathbf{X} \in \mathbb{R}^{p \times N}$ to 
take an step of gradient descent, take small subsets of 
$\mathbf{X}; \mathbf{X_{mini}} \in \mathbb{R}^{p \times 1000}$
at every iteration as the training set.
\begin{marginfigure}% [-1\baselineskip]  % Vertical offset of 1 line,
  \centering
  %\captionsetup{justification=centering}
  \includegraphics[width=\textwidth]{00_figures/costFunction}
  \caption{Using mini-batch makes the cost function not to be monotonically decreasing.}
  \label{fig:costFunction}
\end{marginfigure}
One \emph{epoch} in deep learning means making one
pass (forward and backward propagation) through  
the training set. If we divide the training set into 500
mini-batches, with one epoch we take 500 steps
for gradient descent.

If mini-batch size is 1 then we get stochastic gradient descent.
Here we lose vectorization advantage. Typical sizes for mini-batch 
are 64, 128, 256, 512. 


\subsection{Fast Optimization Methods}
There are methods that converge faster than mini-batch gradient descent:
\begin{itemize}
\item Gradient descent with momentum,
\item RMSprop,
\item Adam optimization algorithm.
\end{itemize}

Let $\{a_t\}$ be a sequence. In order to get a smooth version of the
sequence we can compute $v_t = \frac{\beta v_{t-1} + (1-\beta) a_t}{1-\beta^t}$
that is called \emph{exponentially weighted averages}\marginnote[-1\baselineskip]{This is from 
Deep Learning Specialization course. The second course (improving the neural networks), week 2.}. 
The denominator
corrects for the bias we have at the beginning of the sequence. This is the basis
for the next optimization algorithms.

\textbf{Gradient descent with momentum} improves the convergence of
gradient descent. It can prevent oscillation in some directions.
In this method we will have the following for iteration $t$:

\begin{tcolorbox}
%  \textbf{Algorithm for gradient descent with momentum is given below}
  \begin{algorithm}[H]
\SetAlgoLined
\KwResult{Faster convergence}
$\mathbf{V_{d_w}} = \mathbf{0}, \mathbf{V_{d_b}} = \mathbf{0}  $\;
 \While{at iteration $t$}{
  compute $\mathbf{dW, db}$ on current (mini-)batch\;
  $\mathbf{V}_{d_w} = \beta \mathbf{V}_{d_w} + (1-\beta) {d\mathbf{W}} $\;
  $\mathbf{V}_{d_b} = \beta \mathbf{V}_{d_b} + (1-\beta) {d\mathbf{b}} $\;
  $\mathbf{W} = \mathbf{W} - \alpha \mathbf{V}_{d_w}$\;
  $\mathbf{b} = \mathbf{b} - \alpha \mathbf{V}_{d_b}$\;
 }
 \caption{gradient descent with momentum}
\end{algorithm}
Usually $\beta=0.9$ is a good option.
\end{tcolorbox}

\textbf{RMSprop} is another optimization algorithm that
similarly tries to minimize oscillations:
\begin{tcolorbox}
%  \textbf{Algorithm for gradient descent with momentum is given below}
  \begin{algorithm}[H]
\SetAlgoLined
\KwResult{Faster convergence}
$\mathbf{s_{d_w}} = \mathbf{0}, \mathbf{s_{d_b}} = \mathbf{0}  $\;
 \While{While condition}{
  compute $\mathbf{dW, db}$ on current (mini-)batch\;
  $\mathbf{S}_{d_w} = \beta \mathbf{S}_{d_w} + (1-\beta) {d\mathbf{W}}^2 $\;
  $\mathbf{S}_{d_b}  = \beta \mathbf{S}_{d_b} + (1-\beta) {d\mathbf{b}}^2 $\;
  $\mathbf{W} =\mathbf{W} - \alpha \frac{d\mathbf{W}}{\epsilon + \sqrt{s_{d_w}}}$\;
  $\mathbf{b} = \mathbf{b} - \alpha \frac{d\mathbf{b}}{\epsilon +  \sqrt{\mathbf{S}_{d_b}}}$\;
   }
 \caption{RMSprop algorithm}
\end{algorithm}
${d\mathbf{W}}^2$ and ${d\mathbf{b}}^2$ are element-wise. $\epsilon$ is to avoid dividing by zero.\\
Usually $\beta=0.9$ is a good option.
\end{tcolorbox}
\marginnote[-1\baselineskip]{The video says $d\mathbf{W}$ and $d\mathbf{b}$
however, I am suspicious it should be $\mathbf{S}_{d_w}$ and $\mathbf{S}_{d_b}$.}

\textbf{Adams optimization} combines gradient descent with momentum
and the RMSprop:
\begin{tcolorbox}
%  \textbf{Algorithm for gradient descent with momentum is given below}
  \begin{algorithm}[H]
\SetAlgoLined
\KwResult{Faster convergence}
$\mathbf{S_{d_w}}=\mathbf{0},~ \mathbf{V_{d_w}}=\mathbf{0},~ \mathbf{S_{d_b}} = \mathbf{0},~\mathbf{V_{d_b}} = \mathbf{0}  $\;
 \While{at iteration $t$}{
 
  compute $d\mathbf{W}, d\mathbf{b}$ on current (mini-)batch\;
  $\mathbf{V}_{d_w} = \beta_1 \mathbf{V}_{d_w} + (1-\beta_1) {d\mathbf{W}}$\;
  $\mathbf{S}_{d_w} = \beta_2 \mathbf{S}_{d_w} + (1-\beta_2) {d\mathbf{W}}^2$\;
  \vspace{.2in}
  $\mathbf{V}_{d_b}  = \beta_1 \mathbf{V}_{d_b} + (1-\beta_1) {d\mathbf{b}}$\;
  $\mathbf{S}_{d_b}  = \beta_2 \mathbf{S}_{d_b} + (1-\beta_2) {d\mathbf{b}}^2$\;
  \vspace{.2in}
  $\mathbf{V}_{d_w}^{corrected} = \mathbf{V}_{d_w}/(1-\beta_1^t),
  \mathbf{V}_{d_b}^{corrected} = \mathbf{V}_{d_b}/(1-\beta_1^t)$\;
  $\mathbf{S}_{d_w}^{corrected} = \mathbf{S}_{d_w}/(1-\beta_2^t), ~~
  \mathbf{S}_{d_b}^{corrected} = \mathbf{S}_{d_b}/(1-\beta_2^t)$\;
  \vspace{.2in}
  
  $\mathbf{W} = \mathbf{W} - \alpha \frac{\mathbf{V}_{d_w}^{corrected}}{\epsilon + \sqrt{\mathbf{S}_{d_w}^{corrected}}}$\;
  $\mathbf{b} =  \mathbf{b} - \alpha \frac{\mathbf{V}_{d_b}^{corrected}}{\epsilon +  \sqrt{\mathbf{S}_{d_b}^{corrected}}}$\
 }
 \caption{Adams algorithm}
\end{algorithm}
 Usually $\beta_1=0.9$, $\beta_2=0.999$ are a good options, and $\alpha$ needs
 to be tuned.
\end{tcolorbox}

\subsection{Hyperparameter Tuning}
There are a few hyperparameters that needs to be tuned
in a machine/deep learning process:
\begin{enumerate}
  \item Learning rate $\alpha$
  %------------------------------------------
  \item Momentum $\beta \sim 0.9$ 
  \item Mini batch size
  \item Number of hidden units
  %------------------------------------------
  \item Number of layers
  \item learning rate decay
   %------------------------------------------
  \item Adam optimization parameters: $\beta_1=0.9$ and $\beta_2=0.999$, and $\epsilon=10^{-8}$
\end{enumerate}
The order of importance is as listed above. Item 1 is most important.
Items 2-4 are second priority. Items 5 and 6 are third priority.
Item 7 is almost never tuned.

In order to search the hyperparameter space, use
random values and do not use a grid. The reason is that
if one of the hyperparameters is not too important (e.g. $\epsilon$ in the demominator of Adam's optimization)
then it would not make a difference to try 1, 2, 3, 4, or 5.
And if we search on a grid (See Fig.~\ref{fig:Gridsearch}) then
we have looked into 25 different points of which 5 are actually making a difference,
the ones related to learning rate. But if we do a random search then we have had
25 different values for the learning rate $\alpha$.

\begin{figure}
\centering
\subfloat[]{\includegraphics[width=0.5\textwidth]{00_figures/grid_2D}}
\subfloat[]{\includegraphics[width=0.5\textwidth]{00_figures/random_2D}}
\caption[][11\baselineskip]{Grid search vs random search.
\label{fig:Gridsearch}}
\end{figure}

One good strategy to narrow down the space of hyperparameters
is coarse to fine approach. After finding the best parameter out of 25,
zoom in and search for more points around the best parameters found so far.



