\chapter{Summary for Interviews}
\label{chap:Summary_for_Interviews}

\section{Introduction}
\label{sec:Summary_for_Interviews_Introduction_Interview}

Evaluation Metrics:
\begin{table}[h!]
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Reg.   & RSS    & $R^2$     & Mean Abs. Err. & Median Abs. Err. &           \\ \hline
Class. & accuracy & Class. Err. & Precision $\frac{TP}{TP+FP}$ & Recall   $\frac{TP}{TP+FN}$  & Specifity $\frac{FP}{FP+TN}$ \\ \hline
\end{tabular}
\end{table}

\begin{deff}{}
\textbf{Precision} Of all instances we predict $\hat y = 1$,
what fraction is actually 1.
\begin{equation}\label{eq:precision}
Precision =  \frac{TP}{TP + FP}
\end{equation}
\end{deff}

\begin{deff}{}
\textbf{Recall} Of all instances that are actually $y = 1$,
what fraction we predict 1.
\begin{equation}\label{eq:recall}
Recall =  TPR = \frac{TP}{TP + FN}
\end{equation}
\end{deff}

Precision is important in a task that it is important
to avoid false positive. i.e., it is fine if not all TP instances are detected,
but when the classifier predicts the positive, we want to be very confident.
e.g. predict when to show a query to a customer.

Recall examples: Cancer detection, call customers who want/may
buy, but it is OK if they do not buy.

\begin{deff}{}
\textbf{Specifity} Fraction of all negative instances that 
are incorrectly predicted positive.
\begin{equation}\label{eq:specifity}
Specifity = FPR = \frac{FP}{TN + FP}
\end{equation}
\end{deff}
 Lowering the threshold of classification
 will increase/improve sensitivity and decrease
 

\begin{deff}{}
Adjust $\beta$ for trade off between  precision and recall.
For recall oriented tasts $\beta = 2$ and for precision oriented task $\beta = 0.5$.

\begin{gather} \label{eq:Fscore}% gather and aligned leads to having one label for eq.
\begin{aligned} 
F_\beta &= (1+\beta^2) \times \frac{\text{precision} \times \text{recall}}{(\beta^2 \times \text{precision}) + \text{recall}}\\
             &= \frac{(1+\beta^2) TP}{ (1+\beta^2) TP + \beta^2 FN + FP}\\ 
\end{aligned} 
\end{gather} %

\begin{equation}\label{eq:F1score}
F_1 = \frac{2 \times recall \times precision}{ recall + precision}
\end{equation}
\end{deff}


\begin{deff}{}
Let $d$ be the number of inputs or basis functions in a linear fit
\marginnote{Some of this are from Section 7.4 of~\citep{Hastie2001elements}. They also say
effective number of parameters is based on (or perhaps inspired by) 
$\sum Cov(y_i, \hat y_i)^2 = d\sigma^2$}:
\begin{itemize}
\item AIC for models with least square lost function:
\begin{equation}\label{eq:AICLS}
AIC = \frac{1}{N \hat \sigma^2} (RSS + 2d \hat \sigma^2)
\end{equation}

\item AIC for logistic regression model, using the binomial loglikelihood
\begin{equation}\label{eq:AICLogistic}
AIC = -\frac{2}{N}loglik+ 2\frac{d}{N}
\end{equation}
where $loglik = \sum_1^N log(p_{\hat \theta}(y_i))$ and $\hat \theta$ is
maximum-likelihood estimate of $\theta$.
To use AIC for model selection, we simply choose the model giving 
smallest AIC over the set of models considered. 
For nonlinear and other complex models, we need to replace d by 
some measure of model complexity.
\begin{equation}\label{eq:AICLogisticnonlinear}
AIC(\alpha) = \overline{err}  + 2\frac{d(\alpha)}{N}
\end{equation}

\item The Bayesian information criterion (BIC), like AIC, 
is applicable in settings where the fitting is carried out by 
maximization of a log-likelihood. The generic form of BIC is
\begin{equation}\label{eq:BICGeneral}
BIC = -2 loglik + d log(N)
\end{equation}
Under Gaussian models and squared error loss
\begin{equation}\label{eq:BICGussianSE}
BIC = \frac{N}{\sigma^2} [ \overline{err} + log(N) \frac{d}{N}\sigma^2]
\end{equation}
Therefore BIC is proportional to AIC ($C_p$), with the factor 2 
replaced by $log(N)$. Assuming $N > e^2$, BIC tends to 
penalize complex models more heavily, giving preference 
to simpler models in selection.

\item Under squared error loss
\begin{equation}\label{eq:CPSE}
C_p = \overline{err} + 2\frac{d}{N}\hat \sigma^2
\end{equation}

\end{itemize}
\end{deff}


\noindent \textbf{{{\color{red}{Ada Boost}}}}
Start with $\alpha_i=1/N$. For $t=1, \dots, T$ 
\begin{enumerate}
\item learn $f_t$ with $\alpha_i$
\item compute $\hat w_t = \frac{1}{2} ln \left(  \frac{1-W_{err}(f_t)}{W_{err}(f_t)}   \right)$
where  $W_{err} = \frac{   \sum \alpha_i \mathbb{1}(\hat y_i \neq y_i)  }{\sum \alpha_i}$
\item recompute 
$
\alpha_i^{(t+1)} =  \begin{cases} 
  \alpha_i^{(t)} exp(-\hat w_t),  ~~~~~  f_t(x)= y_i\\
  \alpha_i^{(t)} exp(\hat w_t),    ~~~~~~~ f_t(x) \neq y_i\\
\end{cases}
$
\item Final decision $\hat y = sign(\sum_1^T \hat w_t f_t(x))$
\end{enumerate}

\section{Bias and Variance}
\label{sec:Bias_and_Variance_interview}
Bias measures how on average the model
matches the target. It is an error from
erroneous assumptions. Variance measures
how much the learning algorithm predictions
fluctuates for different training set.
It is the error from sensitivity to small changes 
in the training set.

\[ E[Y - \hat Y] = E[f(x) + \varepsilon - \hat f(x)] = E[f(x) - \hat f(X)]^2 + var(\varepsilon) \]

\[ E[Y_0 - \hat Y_0] =  var(\hat f(x_0)) + 
[\text{bias}((\hat f(x_0))]^2 + var(\varepsilon) \]


\section{$P$-value}
\label{sec:P_value_Interview}

In null hypothesis significance testing, the $p$-value is the probability 
of obtaining test results at least as extreme as the results 
actually observed, under the assumption that the null hypothesis is 
correct. A very small $p$-value means that such an extreme observed 
outcome would be very unlikely under the null hypothesis.
\hl{Smaller $p$-values are taken as stronger evidence against the null hypothesis}.

Let $X$ be a random variable and $x_1, x2, \dots, x_N$ be $N$ realizations.
$\bar x$ then is a random variable and it has expected value and variance:
$E[\bar X ] = E[X] = \mu$; unbiasedness. Variance of $\bar X$ has its own name; standard error.
$SE^2[\bar X] = var(\bar X) = \sigma^2/N$. We do not know $\sigma$ and we should
estimate it. Originally we had $x_1, x2, \dots, x_N$. This set has sample variance;
$S^2 = \frac{1}{N-1}\sum(x_i - \bar x)^2$ is a function of samples $x_i$'s.
So, it has expected value and its own distribution; $E[S^2] = \sigma^2$. The sample
variance $S^2$ is centered at $\sigma^2$. The larger the $N$ the more
concentration about $\sigma^2$. So, mean of $S^2$ is $\sigma^2$. Therefore,
we can use $S^2$ as an estimate of $\sigma^2$ in $SE^2[\bar X] = \frac{\sigma^2}{N}\approx \frac{S^2}{N}$. Hence, $SE[\bar X] = \frac{S}{\sqrt{N}}$.

\begin{itemize}
\item sample mean estimates population mean.
\item sample variance $S^2$ estimates population variance.
\end{itemize}
Variance of $\bar X$ tells us how far, on average, we are from $\mu$ 
if we use a sample set of $x_1, x2, \dots, x_N$ to guess $\mu$.
This gives confidence interval and $p$-value.

Again, $S\approx \sigma$ how variable the population is, and $SE$: how variable
the averages are.

Assuming $X \sim N(\mu, \sigma^2)$ then $Z = \frac{X-\mu}{\sigma^2} \sim N(0, 1)$
and we have a table for $N(0, 1)$ to use for confidence intervals.
We want to know how terrible/good is our $\bar X$ in guessing $\mu$; $\bar X \sim N(\mu, \frac{\sigma}{\sqrt{N}}) \approx \frac{S}{\sqrt{N}}$.
CLT says $\bar X \sim N(\mu, \frac{\sigma^2}{N})$ or $\frac{\bar X - \mu}{  \sqrt{\frac{\sigma^2}{N}} } \sim N(0, 1)$.

Using the table for a given set of instances $x_1, x2, \dots, x_N$ we can
be sure $\bar x$ is in $\bar x \pm   \frac{2\sigma}{\sqrt{n}}$ with 95\% confidence, i.e.
\begin{equation*}
p \left ( \bar x > \mu+\frac{2\sigma}{\sqrt{N}} or \bar x < \mu+\frac{2\sigma}{\sqrt{N}}  \right) = 0.05
\end{equation*}
So, the 0.05 is the $p$-value: probability of seeing $\bar x$ or more extreme.

\begin{table}[h!]
\begin{tabular}{|l|l|l|}
\hline
Truth & Decision &                        \\ \hline
$H_0$ & $H_0$    & Correctly accept $H_0$ \\ \hline
$H_0$ & $H_a$    & Type I error (also FP)      \\ \hline
$H_a$ & $H_a$    &                        \\ \hline
$H_a$ & $H_0$    & Type II error (also FN)  \\ \hline
\end{tabular}
\end{table}

\begin{exm}{}
Suppose $\bar x = 32$ with $S=10$. 
We want to test $H_0$; $\mu=30$, $H_a$: $\mu>30$.
Reject $H_0$ if $\bar x > C$ for some $C$. $C$ takes into account variability of $\bar x$.
Choose C so that probability type I error is $\alpha = 0.05$. 
$\alpha = prob(\text{type I error}) = p(\text{rejecting } H_0 \text{ while it is correct})$

$p(\bar x > C; H_0) = 0.05$ if $C = \mu +\sigma = 31.645$ or $(\bar x - \mu_0) / \left ( \frac{S}{\sqrt{N}} \right) > z_{1-\alpha}$

$p$-value: probability of observing evidence/statistic/estimator as extreme or more under $H_0$.
$p$-value is small means either $H_0$ is false or $H_0$ is true 
and we observed something unlikely.
\end{exm}



Look for $$t = {\bar d} \times \left(\frac{s_d}{\sqrt{N}}\right)^{-1}$$

\section{Coefficient Accuracy}
\label{sec:Coefficient_Accuracy_Interview}
In linear regression
\begin{equation}
\bm \hat \beta  = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}\sim N(\bm \beta , (\mathbf{X}^T \mathbf{X})^{-1} \sigma^2)
\end{equation}

\begin{equation}
(N-p-1)\hat \sigma^2 \sim \sigma^2 \chi^2_{N-p-1}
\end{equation}
For $\hat \beta_j$, $z_j = \frac{\hat \beta_j - 0}{ v_j^{1/2} \hat \sigma}$ where
$v_j = (\mathbf{X}^T \mathbf{X})^{-1}_{jj}$. Large $z_j$ reject $H_0 \equiv \beta_j=0$.\\

Suppose we are seeking mean of a population $X$
but we only have a sample of size $N$. Then the mean
can be estimated by $\mu \approx \hat \mu = \bar x$.

If we use the sample mean $\hat \mu $ to estimate $\mu$, 
this estimate is unbiased, in the sense that on average, 
we expect $\hat \mu$ to equal  $\mu$. 
What exactly does this mean? It means that on the basis 
of one particular set of observations $x_1, \dots, x_N$ , $\hat \mu$ might overestimate 
$\mu$, and on the basis of another set of observations, $\hat \mu$ 
might underestimate $\mu$. But if we could average a huge number of 
estimates of $\mu$ obtained from a huge number of sets of observations, then 
this average would exactly equal $\mu$. Hence, an unbiased 
estimator does not systematically over- or under-estimate the true parameter.
The same idea applies to estimates of 
\begin{gather} % gather and aligned leads to having one label for eq.
\begin{aligned}
\hat \beta_1 &= \frac{\sum_{i=1}^N (x_i - \bar x) (y_i - \bar y)}{ \sum_{i=1}^N (x_i - \bar x)  } \\
\hat \beta_0 &= \bar y - \hat \beta_1 \bar x
\end{aligned}
\end{gather} 
If we estimate $\beta_0$ and $\beta_1$ on the basis of a particular 
data set, then our estimates won’t be exactly equal to $\beta_0$ and $\beta_1$. 
But if we could average the estimates obtained over a huge 
number of data sets, then the average of these estimates 
would be spot on! 

A natural question is as follows: how accurate is the 
sample mean $\hat \mu$ as an estimate of $\mu$? We have 
established that the average of $\mu$'s over many data sets will be 
very close to $\mu$, but that a single estimate $\hat \mu$ may be a 
substantial underestimate or overestimate of $\mu$. How far off will that 
single estimate of $\hat \mu$ be? 
In general, we answer this question by computing the 
standard error of $\hat \mu$ written as $SE (\hat \mu)$. We have the 
well-known formula $Var(\hat \mu) = SE(\hat \mu)^2 = \frac{\sigma^2}{N}$
where $\sigma$ is the standard deviation of each of the realizations $y_i$ 
of $Y$\marginnote{Of course here the $N$ data points are uncorrelated}.
Roughly speaking, the standard error tells us the average 
amount that this estimate $\hat \mu$ differs from the actual value of $\mu$.
Similarly\marginnote{Assuming errors $\varepsilon_i$ 
for each observation have 
common variance $\sigma^2$ and are uncorrelated},
\begin{equation}
SE(\hat \beta_0)^2 = \sigma^2 \left[  \frac{1}{N} + \frac{\bar x^2}{\sum (x_i - \bar x)^2}    \right],~~
SE(\hat \beta_1)^2 = \sigma^2 \left[  \frac{\sigma^2}{\sum (x_i - \bar x)^2}    \right]
\end{equation}
The $\sigma$ can be estimated by \emph{residual standard error} $RSE = \sqrt{\frac{RSS}{N-2}}$. Moreover, we see the more the data is spread
out the smaller $SE(\hat \beta_1)$ will be. 

Standard errors can be used to compute confidence intervals. 
A 95\% confidence interval is defined as a range of values such that with 95\% 
probability, the range will contain the true unknown value of the parameter. 
The range is defined in terms of lower and upper limits computed from the 
sample of data. A 95\% confidence interval has the following property: 
if we take repeated samples and construct the confidence interval for each 
sample, 95\% of the intervals will contain the true unknown 
value of the parameter. For linear regression, the 95\% 
confidence interval for $\beta_1$
approximately takes the form $\hat \beta_1 \pm 2 SE(\hat \beta_1)$.

Standard errors can also be used to perform hypothesis tests 
on the coefficients. The most common hypothesis test involves testing the null hypothesis of
\begin{center}
$H_0$: There is no relationship between $X$ and $Y$; $\beta_1=0$
\end{center}
To test the null hypothesis, we need to determine whether 
$\hat \beta_1$, our estimate for $\beta_1$, is sufficiently 
far from zero that we can be confident that $\beta_1$ is non-zero. 
How far is far enough? This of course depends on the 
accuracy of $\hat \beta_1$--that is, it depends on $SE(\hat \beta_1)$. 
If $SE(\hat \beta_1)$ is small, then even relatively small values of $\hat \beta_1$ 
may provide strong evidence that $\beta_1 \neq 0$, and hence that there is a relationship 
between $X$ and $Y$. 
In contrast, if $SE(\hat \beta_1)$ is large, then 
must be large in absolute value in order for us to reject the 
null hypothesis. In practice, we compute a t-statistic, given by
\begin{equation}
t = \frac{\hat \beta_1 - 0}{SE(\hat \beta_1)}
\end{equation}
which measures the number of standard\marginnote{in linear regression $z-$statistics is used.} 
deviations that $\hat \beta_1$ is away from zero.
If there really is no relationship between $X$ and $Y$, 
then we expect that the equation above will have a t-distribution with $N-2$ 
degrees of freedom. The t-distribution has a bell shape and 
for values of $N$ greater than approximately 30 it is quite similar 
to the standard normal distribution. Consequently, it is a simple 
matter to compute the probability of observing any number
equal to $\abs{t}$ or larger in absolute value, assuming $\beta_1=0$. We call 
this probability the $p$-value. Roughly speaking, we interpret the 
$p$-value as follows: a small $p$-value indicates that it is unlikely 
to observe such a substantial association between the 
predictor and the response due to chance, in the absence of 
any real association between the predictor and the response. \hl{Hence, if we
see a small $p$-value, then we can infer that there is 
an association between the predictor and the response.
We reject the null hypothesis--that is, we declare a relationship to 
exist between $X$ and $Y$--if the $p$-value is small enough.}
Typical $p-$value cut offs are 5\% and 1\%. 
When $N=30$ these correspond to $t-$statistics 
of around  2 and 2.75, respectively.
\marginnote{$t-$statistics is the number of 
standard deviations $\hat \beta_1$ is away from zero.}

\section{Model Accuracy}
\label{sec:Model_Accuracy_Interview}

\begin{description}
\item [RSE]
The RSE is an estimate of the standard deviation of $\varepsilon$ in $y = f(x) + \varepsilon$. 
\begin{equation}
RSE = \sqrt{\frac{1}{N-2} RSS} = \sqrt{\frac{1}{N-2}  \sum(y_i - \hat y_i)^2}
\end{equation}
The RSE is considered a measure of the lack of fit.
If the predictions obtained using the model are very 
close to the true outcome values--$\hat y_i \approx y_i$-- 
then RSE will be small, and we can conclude that the model fits the data very well. 
On the other hand, if $\hat y_i$ is very far from $y_i$ 
for one or more observations, 
then the RSE may be quite large, indicating that the 
model doesn’t fit the data well.

RSS is absolute measure of fit and it depends on
the scale of $Y$. 


\item [$R^2$ Statistic]
The RSE provides an absolute measure of lack of fit. But since it is 
measured in the units of $Y$, it is not always clear what constitutes a 
good RSE. The $R^2$ statistic provides an alternative measure of fit. 
It takes the form of a proportion--the proportion of variance explained--and 
so it always takes on a value between 0 and 1, and is independent of the scale of $Y$.
\begin{equation}
R^2 = \frac{TSS - RSS}{TSS}
\end{equation}
where $TSS = \sum(y_i - \bar y)^2$.
Small $R^2$ can occur because of either 
linear model was not good or the inherent error $\sigma^2$
is large or both. Interpreting $R^2$, measure of linear relation between 
$X$ and $Y$, is easier than
$RSE$ but still hard and depends on the application.
\marginnote{In case of simple linear regression $RSE = cor^2(X, Y)$.}

\item [Multiple Regression]
Below are some important questions.
\begin{enumerate}
\item Is at least one of the predictors useful in predicting the response?
\item Do all the predictors help to explain $Y$ , or is only a subset of the predictors useful?
\item How well does the model fit the data?
\item Given a set of predictor values, what response value should we predict,
and how accurate is our prediction?
\end{enumerate}
Answers:
\begin{enumerate}
\item Question 1 can be written as $H_0: \beta_1 = \beta_2 = \dots =\beta_p = 0$
and can be answered by F-statistics 
\begin{equation}
F = \frac{(TSS-RSS) / p}{ RSS / (N - p - 1)}
\end{equation}
If the linear model assumptions are correct we can show that
\marginnote{$RSE = \sqrt{RSS / (N-p-1)}$}
\begin{equation}
E[RSS/(N - p - 1)] = \sigma^2
\end{equation}
and that, provided $H_0$ is true 
\begin{equation}
E[(TSS - RSS) / p] = \sigma^2
\end{equation}
Hence, \hl{when there is no relationship between 
the response and predictors, one would expect the 
$F$-statistic to take on a value close to 1.}
On the other hand, if $H_a$ is true, then 
$E{(TSS - RSS)/p} > \sigma^2$, so we expect $F$ to be greater than 1.
How large does the F-statistic need to be before we can reject 
$H_0$ and conclude that there is a relationship? It turns 
out that the answer depends on the values of $N$  and $p$ . 
When $N$ is large, an $F$-statistic that is just a little larger than 1 might 
still provide evidence against $H_0$. In contrast, a larger $F$-statistic is 
needed to reject $H_0$ if $N$ is small.

In $F$-statistic above we are testing $H_0$ that all the coefficients are zero. 
Sometimes we want to test that a particular subset of q of the 
coefficients are zero. This corresponds to a null hypothesis
\begin{equation}
H_0 : \beta_{p-q+1} = \beta_{p-q+2} = \dots = \beta_{p} =0,
\end{equation}
where for convenience we have put the variables chosen for 
omission at the end of the list. In this case we fit a second model 
that uses all the variables except those last q. Suppose that the 
residual sum of squares for that model is $RSS_0$. 
Then the appropriate $F$-statistic is
\begin{equation}
F = \frac{(RSS_0 - RSS) / q}{RSS / (N-p-1)}
\end{equation}
Notice that in Table 3.4, for each individual predictor a 
t-statistic and a $p$-value were reported. These provide information 
about whether each individual predictor is related to the response, 
after adjusting for the other predictors. It turns out that each of 
these is exactly equivalent to the $F$-test that omits that single 
variable from the model, leaving all the others in—i.e. q=1 in (3.24). 
So it reports the partial effect of adding that variable to the model.

Given these individual $p$-values for each variable, why do 
we need to look at the overall $F$-statistic? After all, it 
seems likely that if any one of the $p$-values for the individual 
variables is very small, then at least one of the predictors is 
related to the response. However, this logic is flawed, 
especially when the number of predictors p is large.
For instance, consider an example in which $p = 100$ 
and $H_0~:\beta_1 = \beta_2\dots \beta_p=0$ is true, so no variable is truly 
associated with the response. In this situation, about 5\% 
of the $p$-values associated with each variable (of the type shown in Table 3.4) 
will be below 0.05 by chance. In other words, we expect to see approximately 
five small $p$-values even in the absence of any true association 
between the predictors and the response.8 In fact, it is likely that 
we will observe at least one $p$-value below 0.05 by chance!
Hence, if we use the individual t-statistics and associated $p$-values in 
order to decide whether or not there is any association between 
the variables and the response, there is a very high chance that we 
will incorrectly conclude that there is a relationship. However, the 
F-statistic does not suffer from this problem because it adjusts 
for the number of predictors. Hence, if $H_0$ is true, there is 
only a 5\% chance that the $F$-statistic will result in a $p$-value 
below 0.05, regardless of the number of predictors or the number of observations.

\item Important Variable Detection
As discussed in the previous section, the first step in a 
multiple regression analysis is to compute the F-statistic 
and to examine the associated p- value. If we conclude on 
the basis of that $p$-value that at least one of the predictors is 
related to the response, then it is natural to wonder which are 
the guilty ones! We could look at the individual $p$-values 
as in Table 3.4, but as discussed (and as further explored in Chapter 13), 
if p is large we are likely to make some false discoveries.

So, we select the model using Mallow's $C_p$, or BIC or AIC or
adjusted $R^2$ and plotting residuals.

To select which variables to include in different models and then
compare their $C_p$, we can do Forward selection, Backward selection,
mixed selection.
\begin{itemize}
\item Mallow's CP: $\frac{1}{N} (RSS + 2d \hat \sigma^2)$
\item  AIC: $\frac{1}{N\hat \sigma^2}(RSS + 2d \hat \sigma^2)$
\item  BIC: $\frac{1}{N\hat \sigma^2}(RSS + log(N) d \hat \sigma^2)$
\item  Adjusted $R^2$: $1 - \frac{RSS/(N-d-1)}{TSS/(N-1)}$
\end{itemize}

\item Model fit: RSE and $R^2$. An $R^2$ value close to 1 
indicates that the model explains a large portion of the 
variance in the response variable.
It turns out that  $R^2$ will always increase when more 
variables are added to the model, even if those variables are 
only weakly associated with the response.
$RSE = \sqrt{\frac{1}{N-p-1}RSS}$ Thus, models with more 
variables can have higher RSE if the decrease in RSS is small 
relative to the increase in $p$.

\item Prediction Accuracy:
We can compute a confidence interval in order to determine how close $\hat Y$ 
will be to $f(X)$. Prediction intervals tells us 
how much will $Y$ vary from $\hat Y$.\marginnote{Look at the last paragraph of
the book!\cite{james2013introduction} for an example.}

$estimate \pm  t_{mult} \times SE$
Confidence interval 
\begin{equation}\label{eq:COnfInterval}
\hat y_n \pm t_{(\alpha/2, N-2)} \left[  MSE \times \left(  \frac{1}{N} + \frac{(x_n - \bar x)^2}{\sum(x_i - \bar x)^2}\right)      \right]^2
\end{equation}

Prediction interval for $y_{new} \equiv y_n$
\begin{equation}\label{eq:predInterval}
\hat y_n \pm t_{(\alpha/2,~N-2)} \times \left [MSE \times \left (1 + \frac{1}{N} + \frac{(x_n - \bar x)^2}{\sum(x_i - \bar x)^2}    \right)   \right ]^{1/2}
\end{equation}

Graphical summaries can reveal problems with a 
model that are not visible from numerical statistics.

See 77-83 of Introduction to Statistical 
Learning\cite{james2013introduction} if you want.
\end{enumerate}

\item [Confidence Intervals] to see how close $\hat Y$ is
to $f(X)$. In general confidence interval is defined by
$\bar x \pm z\frac{s}{N}$.

\item [Prediction Intervals] to see how much $Y$ varies from $\hat Y$.
Prediction intervals are wider than confidence intervals.
\end{description}



\section{Clustering Comparison}
\label{sec:Clustering_Comparison_Interview}
\begin{tabular}{llr}
\firsthline
% \cline{1-2}
K-means    & Hierarchical \\
\hline
Need to know K in advance  & Need to know number of clusters in advance  \\
Just numerical data               & Works on categorial data and time series        \\
Sensitive to outliers               & Can be used to detect outliers  \\
Computational cost is good   & Computational cost is high         \\
\lasthline
\end{tabular}

K-means: we want to minimize within group variation:
\begin{equation}
\min_{C_1, \dots, C_K} \sum_{k=1}^K W(C_k)
\end{equation}
where
\begin{equation}
W(C_k) = \frac{1}{\abs{C_k}}\sum_{i, i' \in C_k} \norm{x_i - x_{i'}}^2 = 
\frac{2}{\abs{C_k}}\sum_{i \in C_k} \norm{x_i - \bar x}^2
\end{equation}
Minimizing this is very hard. There are $K^n$ ways to partition the observations 
into $K$ clusters. K-means converges to local minimum.

\begin{description}
\item [Effective] number of parameters of K-means is
not $K$. It is {N/k} and usually is bigger than $p$.
\end{description}

\begin{gather} % gather and aligned leads to having one label for eq.
\begin{aligned} 
& RSS = (\mathbf{ y - X} \bm \beta)^T (\mathbf{ y - X} \bm \beta)  \rightarrow \\ 
& \frac{\partial RSS}{\partial \bm \beta} = \bm X^T (\mathbf{y - X} \bm \beta) = 0  \rightarrow  \\
& \bm \beta = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}.
\end{aligned} 
\end{gather} 

\section{Nearest Neighbor}
We seek a function $f(X)$ for predicting $Y$ given values of the input $X$. 
This theory requires a loss function $L(Y, f(X))$ for penalizing errors in prediction, 
and by far the most common and convenient is squared error 
loss: $L(Y, f(X)) = (Y - f(X))^2$. This leads us to a criterion for choosing $f$,
\begin{equation}
EPE[f] = E[(Y - f(X))^2]
= \int (y - f(x))^2 p(dx, dy)
\end{equation}
By conditioning on $X$ we get 
\begin{equation}
EPE[f] = E_XE_{Y|X}[(Y - f(X))^2|X]
\end{equation}
and it will suffice to minimize EPE pointwise
\begin{equation}
f(x) = argmin_cE{Y|X}[(Y-c)^2|X=x]
\end{equation}
and the solution is 
\begin{equation}
f(x) = E[Y|X=x]
\end{equation}
Thus the best prediction of $Y$ at any point $X=x$ is the 
conditional mean, when best is measured by average squared error.
The nearest-neighbor methods attempt to directly 
implement this recipe using the training data. At each point $x$, 
we might ask for the average of all those $y_i$s with input $x_i=x$. 
Since there is typically at most one observation at any point $x$, we settle for
\begin{equation}
\hat f(x) = Ave(y_i|x_i \in N_k(x))
\end{equation}
Two approximations are happening here:
\begin{itemize}
\item expectation is approximated by averaging over sample data;
\item conditioning at a point is relaxed to conditioning on some region
``close'' to the target point. 
\end{itemize}

\section{Problems}
\label{sec:problems_Interview}
\begin{description}
\item [1. Non-linearity of the response-predictor relationships.]
Residual plots are a useful graphical tool for identifying non-linearity. 
Given a simple linear regression model, we can plot the residuals
$y_i - \hat y_i$, versus the predictor $x_i$.
In the case of a multiple regression model,
since there are multiple predictors, we 
instead plot the residuals versus
the predicted (or fitted) values $\hat y_i$. Ideally, the residual plot will 
show no fitted discernible pattern. The presence of a pattern may indicate a problem with
some aspect of the linear model.
If the residual plot indicates that there are non-linear 
associations in the data, then a simple approach is to use 
non-linear transformations of the predictors, such as $log(X)$, $\sqrt{X}$, and $X^2$, 
in the regression model. In the later chapters of this book, we will 
discuss other more advanced non-linear approaches for 
addressing this issue.
\begin{figure*}[h!]
%  \centering
  \captionsetup{justification=centering}
  \includegraphics[width=0.6\textwidth]{00_figures/Nonlinearity}
  \caption[][-14\baselineskip]{a}%
  \label{fig:Nonlinearity}%
\end{figure*}

\item[2. Correlation of Error Terms]
The standard errors that are computed for the estimated regression 
coefficients or the fitted values are based on the assumption 
of uncorrelated error terms. 
If in fact there is correlation among the error terms, then the estimated 
standard errors will tend to underestimate the true standard errors. As a 
result, confidence and prediction intervals will be narrower than they should be.
As an extreme example, suppose we accidentally doubled our data, 
leading to observations and error terms identical in pairs. 
If we ignored this, our standard error calculations would be 
as if we had a sample of size $2N$, 
when in fact we have only n samples. Our estimated parameters 
would be the same for the $2N$ samples as for the $N$ samples, 
but the confidence intervals would be narrower by a factor of $\sqrt{2}$!

In many cases, observations that are obtained at adjacent time 
points will have positively correlated errors. In order to 
determine if this is the case for a given data set, we can plot 
the residuals from our model as a function of time. If the errors 
are uncorrelated, then there should be no discernible pattern. 
On the other hand, if the error terms are positively correlated, 
then we may see tracking in the residuals—that is, adjacent 
residuals may have similar values. Figure~\ref{fig:ErrCorrelation} 
provides an illustration. In the top panel, we see the residuals from 
a linear regression fit to data generated with uncorrelated errors. 
There is no evidence of a time-related trend in the residuals. 
In contrast, the residuals in the bottom panel are from a data 
set in which adjacent errors had a correlation of 0.9. Now there 
is a clear pattern in the residuals—adjacent residuals tend to take 
on similar values. Finally, the center panel illustrates a more 
moderate case in which the residuals had a correlation of 0.5. 
There is still evidence of tracking, but the pattern is less clear.
\begin{figure*}[h!]
%  \centering
  \captionsetup{justification=centering}
  \includegraphics[width=0.6\textwidth]{00_figures/ErrCorrelation}
  \caption[][-14\baselineskip]{Plots of residuals from simulated time series data 
  sets generated with differing levels of correlation $\rho$ between 
  error terms for adjacent time points.}%
  \label{fig:ErrCorrelation}%
\end{figure*}
Many methods have been developed to properly take account of 
correlations in the error terms in time series data. 
Correlation among the error tracking terms can also occur 
outside of time series data. For instance, consider a study in
 which individuals’ heights are predicted from their weights. 
 The assumption of uncorrelated errors could be violated if 
 some of the individuals in the study are members of the same 
 family, eat the same diet, or have been exposed to the same 
 environmental factors. In general, the assumption of uncorrelated 
 errors is extremely important for linear regression as well as for 
 other statistical methods, and good experimental design is crucial in 
 order to mitigate the risk of such correlations.

\item [3. Non-constant Variance of Error Terms]
Another important assumption of the linear 
regression model is that the error terms have a 
constant variance, $var(\varepsilon_i) = \sigma^2$. The standard errors, 
confidence intervals, and hypothesis tests associated 
with the linear model rely upon this assumption.
Unfortunately, it is often the case that the variances of 
the error terms are non-constant. For instance, the 
variances of the error terms may increase with the value 
of the response. One can identify non-constant 
variances in the errors, or heteroscedasticity, 
from the presence of a funnel shape in the residual plot.
An example is shown in the left-hand panel of Figure~\ref{fig:nonConstantVariance}, 
in which the magnitude of the residuals tends to increase 
with the fitted values. When faced with this problem, one 
possible solution is to trans- form the response Y using a 
concave function such as $log(Y)$ or $\sqrt{Y}$. Such a transformation 
results in a greater amount of shrinkage of the larger 
responses, leading to a reduction in heteroscedasticity.
\begin{figure*}[h!]
%  \centering
  \captionsetup{justification=centering}
  \includegraphics[width=0.8\textwidth]{00_figures/nonConstantVariance}
  \caption[][-14\baselineskip]{Left:The least squares regression line is 
  shown in red, and the regression line after removing the outlier is shown 
  in blue. Center: The residual plot clearly identifies the outlier. Right: 
  The outlier has a studentized residual of 6; typically we expect values between -3 and 3.}%
    \label{fig:nonConstantVariance}%
\end{figure*}
Sometimes we have a good idea of the variance of each response. 
For example, the ith response could be an average of $n_i$ 
raw observations. If each of these raw observations is uncorrelated 
with variance $\sigma^2$, then their average has variance $\sigma^2 = \sigma^2 /n_i$. 
In this case a simple remedy is to fit our model by weighted least squares


\item [4. Outliers]
An outlier is a point for which $y_i$ is far from the value predicted 
by the model. Outliers can arise for a variety of reasons, such as 
incorrect recording of an observation during data collection.
The red point (observation 20) in the left-hand panel of Figure~\ref{fig:outlier}
illustrates a typical outlier. The red solid line is the least 
squares regression fit, while the blue dashed line is the 
least squares fit after removal of the outlier. In this case, 
removing the outlier has little effect on the least squares line: 
it leads to almost no change in the slope, and a miniscule 
reduction in the intercept. It is typical for an outlier that 
does not have an unusual predictor value to have little 
effect on the least squares fit. However, even if an outlier 
does not have much effect on the least squares fit, it can 
cause other problems. For instance, in this example, the 
RSE is 1.09 when the outlier is included in the regression, 
but it is only 0.77 when the outlier is removed. Since the 
RSE is used to compute all confidence intervals and $p$-values, 
such a dramatic increase caused by a single data point can 
have implications for the interpretation of the fit. Similarly, 
inclusion of the outlier causes the $R^2$ to decline from 0.892 to 0.805.
\begin{figure*}[h!]
%  \centering
  \captionsetup{justification=centering}
  \includegraphics[width=0.7\textwidth]{00_figures/outlier}
  \caption[][-7\baselineskip]{Left: Observation 41 is a high leverage point, 
  while 20 is not. The red line is the fit to all the data, and the blue line is the 
  fit with observation 41 removed. Center: The red observation is 
  not unusual in terms of its $X_1$ value or its $X_2$ value, but still 
  falls outside the bulk of the data, and hence has high leverage. 
  Right: Observation 41 has a high leverage and a high residual.}%
    \label{fig:outlier}%
\end{figure*}
Residual plots can be used to identify outliers. In this 
example, the out- lier is clearly visible in the residual plot illustrated 
in the center panel of Figure~\ref{fig:outlier}. But in practice, it can be 
difficult to decide how large a residual needs to be before we 
consider the point to be an outlier. To address this problem, 
instead of plotting the residuals, we can plot the \hl{studentized 
residuals, computed by dividing each residual $e_i$ by 
its estimated standard error. Observations whose studentized 
residuals are greater than 3 in absolute value are possible 
outliers.} In the right-hand panel of Figure~\ref{fig:outlier}, the outlier's 
studentized residual exceeds 6, while all other observations 
have studentized residuals between -2 and 2.
If we believe that an outlier has occurred due to an error in data 
collection or recording, then one solution is to simply remove the 
observation. However, care should be taken, since an outlier 
may instead indicate a deficiency with the model, such as a missing predictor.

\item [5. High Leverage Points]
In fact, high leverage observations tend to have a 
sizable impact on the estimated regression line.
It is cause for concern if leverage the least squares line is heavily 
affected by just a couple of observations, because any problems 
with these points may invalidate the entire fit. For this reason, 
it is important to identify high leverage observations.
In a simple linear regression, high leverage observations 
are fairly easy to identify, since we can simply look for 
observations for which the predictor value is outside of 
the normal range of the observations. But in a multiple 
linear regression with many predictors, it is possible to 
have an observation that is well within the range of each 
individual predictor’s values, but that is unusual in terms 
of the full set of predictors.
\begin{figure*}[h!]
%  \centering
  \captionsetup{justification=centering}
  \includegraphics[width=0.7\textwidth]{00_figures/highLeverage}
  \caption[][-7\baselineskip]{Left: Observation 41 is a 
  high leverage point, while 20 is not. The red line is the fit 
  to all the data, and the blue line is the fit with observation 
  41 removed. Center: The red observation is not unusual 
  in terms of its $X_1$ value or its $X_2$ value, but still falls outside 
  the bulk of the data, and hence has high leverage. 
  Right: Observation 41 has a high leverage and a high residual.}%
    \label{fig:highLeverage}%
\end{figure*}

In order to quantify an observation’s leverage, we 
compute the leverage statistic.

The leverage statistic $h_i$ is always between $\frac{1}{n}$ and 1, 
and the average leverage for all the observations is always equal 
to $(p + 1)/n$. So if a given observation has a leverage statistic 
that greatly exceeds $(p+1)/n$, then we may suspect that 
the corresponding point has high leverage.

\item [6. Collinearity] Collinearity refers to the 
situation in which two or more predictor variables are closely related to one another.
The presence of collinearity can pose problems in 
the regression context, since it can be difficult to 
separate out the individual effects of collinear variables on the response.
In other words, since limit and rating tend to 
increase or decrease together, it can be difficult to 
determine how each one separately is associated with the 
response.\marginnote{Think about contours of RSS and gradient descent.}

Since collinearity reduces the accuracy of the 
estimates of the regression coefficients, it causes 
the standard error for $\hat \beta_j$ to grow. Recall that the 
$t-$statistic for each predictor is calculated by 
dividing $\hat \beta_j$ by its standard error. 

Consequently, collinearity results in a decline 
in the $t$-statistic. As a result, in the presence of 
collinearity, we may fail to reject $H_0: \beta_j = 0$
This means that the power of the hypothesis test--the 
probability of correctly detecting a non-zero coefficient--is 
reduced by collinearity.

A simple way to detect collinearity is to look at the 
correlation matrix of the predictors. An element of this matrix 
that is large in absolute value indicates a pair of highly correlated 
variables, and therefore a collinearity problem in the data. 
Unfortunately, not all collinearity problems can be detected by 
inspection of the correlation matrix: it is possible for collinearity 
to exist between three or more variables even if no pair 
of variables has a particularly high correlation. We call this 
situation multicollinearity.
Instead of inspecting the correlation matrix, a better 
way to assess multi-collinearity is to compute the 
variance inflation factor (VIF). The VIF is the ratio of 
the variance of $\hat \beta_j$ when fitting the full model divided by 
the variance of $\hat \beta_j$ if fit on its own. The smallest 
possible value for VIF is 1, which indicates the complete 
absence of collinearity. Typically in practice there is a small 
amount of collinearity among the predictors. As a rule of thumb, 
a VIF value that exceeds 5 or 10 indicates a problematic 
amount of collinearity. The VIF for each variable can be 
computed using the formula
\begin{equation}
\label{eq:VIF}
VIF(\hat \beta_j) =  \left(1 - R^2_{X_j|X_{-j}} \right)^{-1}
\end{equation}
where $R^2_{X_j|X_{-j}}$ is is the $R62$ from a regression of
$X_j$ onto all of the other predictors. If $R^2_{X_j|X_{-j}}$
is close to 1, then collinearity is present, and so VIF will be large.

When faced with the problem of collinearity, there are two 
simple solutions. The first is to drop one of the problematic 
variables from the regression. This can usually be done 
without much compromise to the regression fit, since the 
presence of collinearity implies that the information that this 
variable provides about the response is redundant in the 
presence of the other variables. The second solution is to combine 
the collinear variables together into a single predictor. 
For instance, we might take the average of standardized versions.

I will stop here, read pages 98-103 of Introduction to Statistical Learning\cite{james2013introduction}.
\end{description}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%
%%%%%
%%%%%
\section{Bootstrap}
Bootstrap is a general tool for assessing statistical accuracy.
First we describe the bootstrap in general, and then show how it 
can be used to estimate extra-sample prediction error. 
As with cross-validation, the boot- strap seeks to estimate 
the conditional error $Err_\tau$ , but typically estimates well 
only the expected prediction error $Err$.


Suppose we have a model fit to a set of training data.
Denote the training set by $\tau = \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$
The basic idea is to draw samples with replacement from $\tau$ and
form a set of training sets each of which having the same size as $\tau$.
Suppose we have $B$ such sets; size of bootstrap is $B$.
We fit the model to each of the bootstrap dataset and
examine behavior of the fits over the $B$ replications.

In the~\cref{fig:bootstrap}, $S(Z)$ is any quantity computed 
from the data $Z$, for example, the prediction at some input point. 
From the bootstrap sampling we can estimate any 
aspect of the distribution of $S(Z)$, for example, its variance,
\begin{equation}
\label{eq:bootVariance}
\widehat{Var}[S(Z)] = \frac{1}{B-1} \sum_{b=1}^B (S(Z^{*b}) - \bar S^*)^2,
\end{equation}
where $\bar S^* = \sum_b S(Z^{*b})/B$. Note that $\widehat{Var}[S(Z)]$ 
can be thought of as a Monte-Carlo estimate of the variance $S(Z)$
under sampling from the empirical distribution function $\hat F$ for the
data $\Tau = (Z =) \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N) \}$.

How can we apply the bootstrap to estimate prediction error? 
One approach would be to fit the model in question on a 
set of bootstrap samples, and then keep track of how well 
it predicts the original training set. If $f^{*b}(x_i)$ is the predicted 
value at $x_i$, from the model fitted to the bth bootstrap dataset, 
our estimate is
\begin{equation}
\widehat Err_{boot} = \frac{1}{B} \frac{1}{N} \sum_{b=1}^B \sum_{i=1}^N
L(y_i, \hat f^{*b}(x_i)).
\end{equation}
However, it is easy to see that $\widehat Err_{boot}$ does not provide a 
good estimate in general. The reason is that the bootstrap 
datasets are acting as the training samples, while the
original training set is acting as the test sample, and these 
two samples have observations in common. This overlap 
can make overfit predictions look unrealistically good, 
and is the reason that cross-validation explicitly uses 
non-overlapping data for the training and test samples.


\begin{figure*}[h!]
%  \centering
  \captionsetup{justification=centering}
  \includegraphics[width=0.6\textwidth]{00_figures/bootstrap}
  \caption[][-14\baselineskip]{Schematic of the bootstrap process. In 
  this plot $\tau$ is replaced with $Z$.
  We wish to assess the  statistical accuracy of a quantity $S(Z)$ 
  computed from our dataset. $B$ training sets $Z^{*b}, ~b = 1, \dots, B$ 
  each of size $N$ are drawn with replacement from the original dataset. 
  The quantity of interest $S(Z)$ is computed from each bootstrap 
  training set, and the values $S(Z^{*1}) \dots S(Z^{*B})$ are 
 used to assess the statistical accuracy of $S(Z)$.}%
  \label{fig:bootstrap}%
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%
%%%%%
%%%%%
\section{Bootstrap Aggregation or Bagging: How to Use Bootstrap}\label{Bagging}
To improve the estimate or prediction itself consider the regression
problem. Suppose we fit a model to our training data 
$\tau = \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$ obtaining prediction
$\hat f(x)$ at point $x$.
Bootstrap aggregation or bagging averages this prediction over a collection of bootstrap samples thereby reducing the variance. For each bootstrap sample
$\tau^b, ~b=1, \dots, B$ we fit a model giving a prediction $\hat f^b(x)$.
The bagging estimate is defined by $\hat f_{bagging}(x) = \frac{1}{B}\sum\hat f^b(x)$.

Bagging is a method for reducing the variance of an estimated prediction. Bagging works well for high-variance low-bias procedures such as trees.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%
%%%%%
%%%%%
\section{Decision Tree, Random Forest, and Extra Tree-Bagging}
\label{sec:Random_Forest_Interview}
Advantages of trees:
\begin{enumerate}
\item Can handle both numerical and categorical variables.
\item Easy to interpret.
\item Internal work can be observed.
\item Performs well on large datasets.
\item Fast
\item Low bias
\end{enumerate}

\noindent Disadvantages of trees:
\begin{enumerate}
\item Choice of good splitting choices at each node; e.g. Hunts algorithm is a good one but does not consider global optimum.
\item Prone to overfitting.(max depth or pruning can help)
\item High variance
\end{enumerate}

Random forest is a substantial modification of bagging
that builds a large collection of de-correlated trees
and averages them. On may problems performance of 
random forests is similar to boosting (coming up next)
while they are simpler to train and tune.

The idea in bagging is to average over many noisy 
but approximately unbiased models and hence reduce the
variance. Trees are ideal for bagging since they can capture
complex relationships and structures and if grow deep will
have low bias.  Since each tree in the bagging is identically
distributed, the expectation of average of $B$ such trees
is the same as the expectation of any one of them. So,
the bias of bagged trees is the same as that of one tree and the hope is
reducing variance. This is in contrast to boosting where
trees are grown in an adaptive way to improve bias and hence are not
identically distributed. 

An average of $B$ i.i.d. random variables each with variance $\sigma^2$
has variance $\frac{1}{B}\sigma^2$. If the variables are simply identically distributed but not necessarily independent, with positive pairwise correlation,
the variance of average is $\rho \sigma^2 + \frac{1-\rho}{B}\sigma^2$.
As $B$ increases the second term vanishes. 
So, in random forest, one tries to reduce variance by reducing
the correlation between trees through random selection of
input variables at each node, specifically when growing a tree
on a bootstrapped dataset.\\

\noindent \hl{\textbf{Summary}}

Random forest is a bagging method to reduce the variance
of the model. It has more flavor, than just bagging, to it.
Trees in a random forest are identically distributed. 
So, expectation of an average of $B$ trees is the same as that of 1 tree.
In other words, the bias of a random forest is the same as 1 tree.
For $B$ i.i.d random variables, the variance is $\frac{\sigma^2}{B}$
where $\sigma^2$ is variance of each. If they are hust i.d. (not independent),
then variance of $B$ of them is $\rho \sigma^2 + \frac{1-\rho}{B} \sigma^2$
where $\rho$ is correlation. Random forest tries to reduce correlation between
trees.

At each node in each tree of a random forest, 
choose $m < p$ ($m=\sqrt{p}$ or $p/3$) variables randomly to do the
splittings. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%
%%%%%
%%%%%
\section{Extra Trees}
At each node the feature to split is chosen randomly
and it uses whole learning sample rather than a bootstrap.
This is done $M$ times. 
So, we have $M$ trees, each tree uses all training set. At each node,
similar to random forest, we choose a random subset of 
features to split on, but the difference is that the cutting points
are chosen randomly. For example, if we have $(a, b)$ interval the
cutting point is chosen randomly; not the optimal cut point.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%
%%%%%
%%%%%
\section{Boosting}
Boosting is different from bagging.
The motivation for boosting was a procedure that
combines weak classifiers to produce a powerful committee.
A weak classifier is one whose error rate is slightly better than
random. The purpose of boosting is to \emph{sequentially}
apply the weak classification algorithm to repeatedly modified versions 
of the data thereby producing a sequence of weak classifiers $G_m(x),~m=1, \dots, M$. The predictions from all of them are then combined through a
weighted majority vote to produce the final prediction; $G(x) = sign \left(\sum_{m=1}^M \alpha_m G_m(x) \right)$. The $\alpha_i$s are
computed by the boosting algorithm and their role is to weight contribution
of each tree to give higher influence power to more accurate trees.
The data modification at each boosting step consist of applying 
weights $w_i$ to each training point  $x_i$. At step $m$
the observations that are misclassified by $G_{m-1}$ have their weights increased so as iteration proceeds the observations that
are hard to classify receive increasing influence so that
the next classifier is forced to focus on those observations.

\begin{tcolorbox}
%  \textbf{Algorithm for gradient descent with momentum is given below}
  \begin{algorithm}[H]
\SetAlgoLined
\KwResult{Powerful output (through reducing bias)}
  Initialize $w_i = \frac{1}{N}$\;
 \For{$m=1$ to $M$}{
  fit a classifier $G_m(x)$ to data using $w_i$s\;
  Compute $err_m = \frac{\sum w_i I(y_i \neq G_m(x_i))}{\sum w_i}$\;
  Compute $\alpha_m = log\frac{1-err_m}{err_m}$\;
  Compute $w_i = w_i ~ exp(\alpha_m I[y_i \neq G_m(x_i)])$ \;
 }
\textbf{Output}: $G(x) = sign \left( \sum_{m=1}^M \alpha_m G_m(x) \right)$
 \caption{Boosting}
\end{algorithm}
\end{tcolorbox}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%
%%%%%   PCA
%%%%%
\section{PCA}
\label{sec:PCA_interview}

Let us begin with sample covariance:
\begin{equation}
Q = \text{sample covariance} = \frac{1}{N-1} \sum_{i=1}^{N}
(x_i - \bar x) (x_i - \bar x)^T
\end{equation}
where $\bar x$ is the average.

PCA pays attention to variances; perhaps there are 
other techniques looking into covariances!
The goal is to find a direction in which the projection
of data into it will have highest variance.
So, we are looking for a map $\alpha$ that
maps the point $x = (x_1, x_2, \dots, x_p)$
into a 1-dimensional subspace where
the set of all such points will have highest variance
amongst all possible 1-dimensional subspaces.
So, we want to find the set of points that look like
\[z_1 = \alpha_1^T x.\]
subject to maximizing the variance 
$var[z_1] = var[\alpha_1^T x] = \alpha_1^T \Sigma \alpha_1$
and $\norm{\alpha_1} = 1$.
The index 1 above is related to the first dimension in the
PCA coordinate; i.e. not the first sample in the training set.
To do so the technique of Lagrange multipliers are used
\[ \alpha_1^T \Sigma \alpha_1 - \lambda (\alpha_1^T\alpha_1 -1)\]
where $\lambda$ is Lagrange multiplier.
Differentiate w.r.t. $\alpha_1$ and setting it to zero gives
\[\Sigma \alpha_1 - \lambda \alpha_1 = 0\]
We can see $\alpha_1$ and $\lambda$ are eigen-pairs
of $\Sigma$; the covariance matrix.

The second direction (basis or PC) in the PCA subspace
will be the direction that is perpendicular to direction of
$\alpha_1$ and the projection of sample points in this space
must have second largest variance amongst all other directions.
Similarly, such direction will be given by another eigen-pair of
$\Sigma$ where $\lambda_2$ is the second largest
eigenvalue of $\Sigma$; $\lambda_2 < \lambda_1$.

Some assumptions made here are that
none of the variances of PC's are equal, nor they
are zero.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%
%%%%%
%%%%%

\section{One Liners}
\begin{itemize}
\item The ridge solutions are not equivariant under scaling of the 
inputs, and so one normally standardizes the inputs before solving.

\item Feature scaling matters  for $K$-means, $K$-NN, SVM, PCA, LDA, logistic regression, NN.

\item Bias measure how on average the model matches the target. Variance measures
how much the model fluctuates for different training sets.
Bias is an error from erroneous assumptions about the model.
Variance is the error from sensitivity to small changes in training set.

\item Gini index (impurity measurement of node/region $R_m$ in a tree)
\begin{equation}\label{eq:GiniIndex}
G = \sum_{k=1}^K \hat p_{mk} (1-\hat p_{mk})
\end{equation}
where $\hat p_{mk} = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i = k)$.
\item Cross-entropy or deviance (impurity measurement)
\begin{equation}\label{eq:entropyIndex}
D = - \sum_{k=1}^K \hat p_{mk} log(\hat p_{mk})
\end{equation}

\item Use Gini index or entropy for growing tree. For pruning use
classification error, if the goal is prediction accuracy.

\item For imbalanced data, use SMOTE (Synthetic Minority Oversampling Technique),
or weight classes, or cost sensitive algorithm.

\item The hierarchical principle states that if we include 
an interaction in a model, we should also include 
the main effects, even if the $p$-values associated with 
their coefficients are not significant. In other words, if the 
interaction between $X_1$ and $X_2$ seems important, then 
we should include both $X_1$ and $X_2$ in the 
model even if their coefficient estimates have large $p$-values.

\item Naive Bayes is naive because it assumes variables are
independent and equally important.

\item TF TF-IDF = TF * IDF where TF is Word Count and IDF is $log(\frac{\text{\# docs}}{1 + \text{ \# docs using the word}})$

\item Distributions used for following events:
    \begin{enumerate}
        \item Probability of $k$ people arrival within a 
        time-window to a place: Poisson
        \item Probability of people's height: Normal
        
        \item Probability of sum of two 6-sided fair dices being $y$
        
        \item Probability of having $h$ heads out of $N$ tries.
        Binomial is number of successes in a fixed number of 
        independent trials. \[\binom{N}{h}\]
        Geometric distribution: time between successes in a series of
        independent trials.
     \end{enumerate}

\item Decision trees are good for non-linear relations. May not 
work well with time series which tend to follow linear trends.

\item Some assumptions of regression:
            \begin{enumerate}
                  \item In simple linear regression: 
                           Linear and additive relation between 
                          dependent and independent variables.
                          One unit change in $x_i$ cases a constant 
                          change in $y$. 
                          
                          \item Errors should not be correlated.
                          
                          \item independent variables should not be correlated.
                          lack of this assumption is known as multi-collinearity.
                          variance inflation factor can be used for diagnosis.
                          
                          \item  Error terms are assumed to have
                          constant variance. 
                          
                          \item Error terms are assumed to be normally
                          distributed.
                  
            \end{enumerate}
            
 \item A Box Cox transformation is a transformation of 
      non-normal dependent variables into a normal shape. 
      Normality is an important assumption for many statistical 
      techniques; if your data isn't normal, applying a Box-Cox 
     means that you are able to run a broader number of tests.
     
     \item Logistic regression evaluation: Precision, Sensitivity, AUC, 
     $R^2$ statistics computed by McFadden or ``Cox and Shell''. 
     Likelihood ratio test, $\chi^2$-tests such as Pearson $\chi^2$.
     
     McFadden $R^2 = 1 - \frac{ln(L_M)}{ln(L_0)}$ where $L_0$ 
     is likelihood of the function with no predictor and $L_M$ is likelihood 
     for the model being evaluated.  $ln(L_0)$ plays the role of RSS.  
     McFadden test corresponds to proportional reduction in 
     error variance. Small McFadden is bad.
     
     Cox and Shell $R^2 = 1 - \left (\frac{L_M}{L_0}\right )^{2/n}$
     \[ln(L_0) = n_0 log(n_0/n) + n_1 log(n_1/n) \]
     \noindent where $n_0$ is number of observations equal to 0,
     $n_1$ is number of observations equal to 1, and $n = n_0+n_1$.
     $L_0$ is the model containing only interception 
     and no regression coefficient.
     
     
     \begin{equation*}
         \begin{aligned}
  \mathcal{L}(\beta_0, \beta_1) &= log(\Pi_{i; y_i=1} p(x_i)) + log(\Pi_{i; y_i=0} [1-p(x_i)])\\ 
               &= \sum_i \{y_i log(p_i) + (1-y_i) log(1-p_i)\}
           \end{aligned}
     \end{equation*} 
     
      Logit is inverse of logistic function; $logit(p) = log(\frac{p}{1-p})$.
      The null model assumes homogeneity. 
      So, the two groups have the same probability and 
      hence the same logit;  $logit (\pi_1) = \eta$, where $\pi_i$ is probability of being 1.
      In the null model we only have the intercept $\beta_0 = ln(\frac{\pi}{1-\pi})$
     
     
     \item Collinearity of predictive variables can cause uncertainty in
              coefficient estimates. A small change in data can cause
              the the pair of coefficients that resulted in small RSS 
              change a lot. It reduces accuracy of estimates 
              of regression coefficients. It causes SE of $\hat \beta$ grow,
              and therefore, lead to a decline in power of $t$-statistic test,
              and therefore, in increase of $p$-value, and hence, we may
              fail to reject null hypothesis.

      \item Get rid of correlated variables before PCA.
      
\end{itemize}


\section{DL One Liners}
\begin{itemize}
\item There is no bias-variance trade-off here. And we can look for problems as follows
\begin{table}[h!]
\caption{NN Diagnosis} \label{tab:NNDiagnosis}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\rowcolor{shadecolor} {\cellcolor{black} } & \multicolumn{4}{c|}{Error Rate}  \\ 
\hline
Train & 1\% & 15\%  & 15\% & 0.5\%  \\ 
\hline
\rowcolor{aliceblue} 
Dev. & 11\%  & 16\%   & 30\% & 1\%    \\ \hline
Problem & var. & bias & bias \& var. & Happy  \\ \hline
\end{tabular}
\end{table}

Recipe for attacking the problems:
\begin{description}
\item [1. High bias?] 
$\boldsymbol \cdot $ Try bigger network
$\boldsymbol \cdot $ Try longer training (GD) or better optimization algorithms.
$\boldsymbol \cdot $ Maybe other architecture

\item [2. No bias but high variance?] 
$\boldsymbol \cdot $ Get more data
$\boldsymbol \cdot $ Regularization
$\boldsymbol \cdot $ Maybe other architecture

\item [3. No bias and No variance?] Happy days.
\end{description}

\item Drop out regularization: For each training example, eliminate some neurons 
in each layer at random. For a given example, this should not be fixed at each epoch.

\item Early stopping can be a form of regularization. In image cases, crop, shift, rotation, 
and flipping can raise number of training examples.

\item Optimization Algorithms\marginnote{Course 2 - Week 3}(the last two are weighted averages methods): 
$\boldsymbol \cdot $ Mini-batch GD
$\boldsymbol \cdot $ GD with momentum
$\boldsymbol \cdot $ RMS-prop
$\boldsymbol \cdot $ Adam optimization

\end{itemize}




