{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbeb8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Applications/Xcode.app/Contents/Developer/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa5f54",
   "metadata": {},
   "source": [
    "At this time, Feb 18, 2026 Torch is not working with Python 3.13. Stop updating too quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d74eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8edfdc",
   "metadata": {},
   "source": [
    "A scalar (a single number) has zero dimensions, a vector has one dimension, a matrix has two dimensions, and a tensor has three or more dimensions. That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9172237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1416)\n",
      "--------------------------------------------------\n",
      "tensor([1, 2, 3])\n",
      "--------------------------------------------------\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--------------------------------------------------\n",
      "tensor([[[-1.9405, -1.4705, -1.9537,  0.9611],\n",
      "         [-0.3476, -0.9362, -1.9944, -0.4377],\n",
      "         [-1.9402, -0.6080,  0.4600,  0.5381]],\n",
      "\n",
      "        [[-0.6142, -0.7736, -0.9027,  0.6159],\n",
      "         [-0.0477,  0.4390, -0.8742,  0.3394],\n",
      "         [-1.3476, -1.6806, -0.0862, -0.5908]]])\n",
      "--------------------------------------------------\n",
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n",
      "--------------------------------------------------\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "\n",
    "print(scalar)\n",
    "L=50\n",
    "print (\"-\"*L)\n",
    "print(vector)\n",
    "print (\"-\"*L)\n",
    "print(matrix)\n",
    "print (\"-\"*L)\n",
    "print(tensor)\n",
    "print (\"-\"*L)\n",
    "print(tensor.size(), tensor.shape)\n",
    "dummy_array = np.array([1, 2, 3])\n",
    "dummy_tensor = torch.as_tensor(dummy_array)\n",
    "# Modifies the numpy array\n",
    "dummy_array[1] = 0\n",
    "# Using numpy function to convert PyTorch tensor to Numpy array\n",
    "print(dummy_tensor.numpy())\n",
    "print(scalar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec328b4",
   "metadata": {},
   "source": [
    "You can also reshape a tensor using its ```view()``` (preferred) or ```reshape()``` methods.\n",
    "\n",
    "**Beware**: The ```view()``` method only returns a tensor with the desired shape that shares the underlying data with the original tensor; it does not create a new, independent tensor!\n",
    "\n",
    "The ```reshape()``` method may or may not create a copy! The reasons behind this (apparently) weird behavior are beyond the scope of this lesson, but this behavior is the reason why ```view()``` is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13bd8290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--------------------------------------------------\n",
      "tensor([[1., 2., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "# We get a tensor with a different shape but it still is the SAME tensor\n",
    "same_matrix = matrix.view(1, 6)\n",
    "# If we change one of its elements...\n",
    "same_matrix[0, 1] = 2.\n",
    "# It changes both variables: matrix and same_matrix\n",
    "print(matrix)\n",
    "print (\"-\"*L)\n",
    "print(same_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597af5be",
   "metadata": {},
   "source": [
    "If you want to copy all data for real, that is, duplicate the data in memory, you may use either its ```new_tensor()``` or ```clone()``` methods.\n",
    "\n",
    "----------\n",
    "\n",
    "The first ```matrix``` below acts as a template. It controls:\n",
    "\n",
    "- Data type (e.g., float32, int64)\n",
    "- Device (CPU or CUDA GPU)\n",
    "- Layout / other tensor settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d1d6fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--------------------------------------------------\n",
      "tensor([[1., 3., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0j/hq0rj1017116ysjtk0p16b540000gn/T/ipykernel_11752/2398196490.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  different_matrix = matrix.new_tensor(matrix.view(1, 6))\n"
     ]
    }
   ],
   "source": [
    "# We can use \"new_tensor\" method to REALLY copy it into a new one\n",
    "different_matrix = matrix.new_tensor(matrix.view(1, 6))\n",
    "\n",
    "# Now, if we change one of its elements...\n",
    "different_matrix[0, 1] = 3.\n",
    "\n",
    "# The original tensor (matrix) is left untouched!\n",
    "# But we get a \"warning\" from PyTorch telling us \n",
    "# to use \"clone()\" instead!\n",
    "print(matrix)\n",
    "print (\"-\"*L)\n",
    "print(different_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c874b",
   "metadata": {},
   "source": [
    "It seems that PyTorch prefers that we use ```clone()``` together with ```detach()``` instead of ```new_tensor()```. Both ways accomplish the same result, but the code below is deemed cleaner and more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fba8b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--------------------------------------------------\n",
      "tensor([[1., 4., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Lets follow PyTorch's suggestion and use \"clone\" method\n",
    "another_matrix = matrix.view(1, 6).clone().detach()\n",
    "\n",
    "# Again, if we change one of its elements...\n",
    "another_matrix[0, 1] = 4.\n",
    "\n",
    "# The original tensor (matrix) is left untouched!\n",
    "print(matrix)\n",
    "print (\"-\"*L)\n",
    "print(another_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3440ed2",
   "metadata": {},
   "source": [
    "### Conversions between Numpy and PyTorch\n",
    "\n",
    "The ```as_tensor``` method preserves the type of the array, which can also be seen in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89bd0682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 torch.float64\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.randn(10)\n",
    "\n",
    "x_train_tensor = torch.as_tensor(x_train)\n",
    "print(x_train.dtype, x_train_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ebf793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x_train_tensor = torch.as_tensor(x_train)\n",
    "float_tensor = x_train_tensor.float() # (going from float64 to float32) \n",
    "print(float_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940aa2d1",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Both ```as_tensor()``` and ```from_numpy()``` return a tensor that shares the underlying data with the original Numpy array. Similar to what happened with ```view()```, if you modify the original Numpy array, you are modifying the corresponding PyTorch tensor too and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb3b4288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "dummy_array = np.array([1, 2, 3])\n",
    "dummy_tensor = torch.as_tensor(dummy_array)\n",
    "\n",
    "dummy_array[1] = 0 # Modifies the numpy array\n",
    "print(dummy_tensor) # Tensor gets modified too..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d2f14",
   "metadata": {},
   "source": [
    "What do we need ```as_tensor()``` for? Why canâ€™t we just use ```torch.tensor()```?\n",
    "\n",
    "Well, you could. Just keep in mind that ```torch.tensor()``` always makes a copy of the data instead of sharing the underlying data with the Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27a1be1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3]\n"
     ]
    }
   ],
   "source": [
    "## Opposite Direction\n",
    "\n",
    "dummy_array = np.array([1, 2, 3])\n",
    "dummy_tensor = torch.as_tensor(dummy_array)\n",
    "# Modifies the numpy array\n",
    "dummy_array[1] = 0\n",
    "# Using numpy function to convert PyTorch tensor to Numpy array\n",
    "print(dummy_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a503b4",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d469a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_cudas = torch.cuda.device_count()\n",
    "for i in range(n_cudas):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3603419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3702, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# if you had GPU, in the print output you would see device='cudaXYZ'\n",
    "gpu_tensor = torch.as_tensor(x_train).to(device)\n",
    "print(gpu_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a9a8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them \n",
    "# into PyTorch's Tensors and then we send them to the  chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "203ad0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> **** <class 'torch.Tensor'> **** torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train), \"****\", type(x_train_tensor), \"****\", x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01456038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# GPU tensor back to NumPy:\n",
    "back_to_numpy = x_train_tensor.numpy() # For CPU\n",
    "back_to_numpy = x_train_tensor.cpu().numpy() # For GPU\n",
    "\n",
    "print(back_to_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763a309",
   "metadata": {},
   "source": [
    "The following would fail; the ```requires_grad``` will be lost if we do it this way.\n",
    "Next cell is the correct way of doing it.\n",
    "\n",
    "```python\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e1daf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58a21e0a",
   "metadata": {},
   "source": [
    "### ```backward```, ```grad```, and ```zero_``` Methods\n",
    "\n",
    "Every time we use the gradients to update the parameters, we need to zero the gradients afterward. And that is what ```zero_()``` is good for. i.e. by default gradients accumulate! we need to set them to zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62419ec1",
   "metadata": {},
   "source": [
    "```python\n",
    "yhat = b + w * x_train_tensor\n",
    "error = (yhat - y_train_tensor)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "# No more manual computation of gradients! \n",
    "# b_grad = 2 * error.mean()\n",
    "# w_grad = 2 * (x_tensor * error).mean()\n",
    "loss.backward()\n",
    "\n",
    "print(b.grad, w.grad)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70237d09",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8303adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "lr = 0.1 \n",
    "\n",
    "optimizer = optim.SGD([b, w], lr=lr)               # 1)\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = b + w * x_train_tensor\n",
    "    error = (yhat - y_train_tensor)\n",
    "    loss = (error ** 2).mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - updates parameters using gradients and \n",
    "    # the learning rate. No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     b -= lr * b.grad\n",
    "    #     w -= lr * w.grad\n",
    "    optimizer.step()                               # 2)\n",
    "    \n",
    "    # No more telling Pytorch to let gradients go!\n",
    "    # b.grad.zero_()\n",
    "    # w.grad.zero_()\n",
    "    optimizer.zero_grad()                          # 3)\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ab395",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Cleaned up\n",
    "###\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "lr = 0.1 \n",
    "\n",
    "optimizer = optim.SGD([b, w], lr=lr)    # 1)\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = b + w * x_train_tensor\n",
    "    error = (yhat - y_train_tensor)\n",
    "    loss = (error ** 2).mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()                    # 2)\n",
    "    optimizer.zero_grad()               # 3)\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1086926",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "predictions = torch.tensor([0.5, 1.0])\n",
    "labels = torch.tensor([2.0, 1.3])\n",
    "print(loss_fn(predictions, labels))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e20b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "loss_fn = nn.MSELoss(reduction='mean')             # 1) MSE loss function\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1 - computes model's predicted output - forward pass\n",
    "    yhat = b + w * x_train_tensor    \n",
    "    loss = loss_fn(yhat, y_train_tensor)           # 2)\n",
    "\n",
    "    # Step 3 - gradients for \"b\" and \"w\"\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - updates parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173a190",
   "metadata": {},
   "source": [
    "Since ```loss``` is computing gradients we need to use ```detach``` to convert it to numpy or ```tolist```:\n",
    "\n",
    "```python\n",
    "print(loss.detach().cpu().numpy())\n",
    "print(loss.tolist())\n",
    "loss.item() # only if there is one scaler.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc70d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
