{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aedb5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f74604",
   "metadata": {},
   "source": [
    "In the ```__init__``` method, we define our two parameters of ```b``` and ```w```, using the ```Parameter()``` class, to tell PyTorch that these tensors, which are attributes of the ```ManualLinearRegression``` class, should be considered parameters of the model the class represents.\n",
    "\n",
    "Why should we care about that? By doing so, we can use our model’s ```parameters()``` method to retrieve an iterator over all model’s parameters, including parameters of nested models. Then we can use it to feed our optimizer (instead of building a list of parameters ourselves)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c29ca",
   "metadata": {},
   "source": [
    "```nn.Module``` is the base class for all neural networks.\n",
    "By inheriting from ```nn.Module```, your class automatically gets:\n",
    "- Parameter tracking\n",
    "- ```.parameters()``` method\n",
    "- ```.to(device)``` support\n",
    "- ```.train()``` / ```.eval()``` modes\n",
    "- Model saving/loading (```state_dict```)\n",
    "- Integration with optimizers\n",
    "- Automatic registration of layers\n",
    "\n",
    "----------\n",
    "\n",
    "```super().__init__()``` calls the parent class’s constructor (here ```nn.Module```) to properly initialize all the internal PyTorch machinery so your model works correctly.\n",
    "\n",
    "\n",
    "PyTorch sets up:\n",
    "- Internal dictionaries to store parameters\n",
    "- Hooks\n",
    "- Buffers\n",
    "- Submodules\n",
    "- Gradient tracking infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6f3b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([0.3367], requires_grad=True), Parameter containing:\n",
      "tensor([0.1288], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"b\" and \"w\" real parameters of the model, \n",
    "        # we need to wrap them with nn.Parameter\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.b + self.w * x\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "# Creates a \"dummy\" instance of our ManualLinearRegression model\n",
    "dummy = ManualLinearRegression()\n",
    "print(list(dummy.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73502ae",
   "metadata": {},
   "source": [
    "**The ```state_dict``` method**\n",
    "\n",
    "Moreover, we can get the current values of all parameters using our model’s ```state_dict()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b16d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([0.3367])), ('w', tensor([0.1288]))])\n"
     ]
    }
   ],
   "source": [
    "ummy = ManualLinearRegression()\n",
    "print(dummy.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7748135",
   "metadata": {},
   "source": [
    "In the following cell, ```yhat = model(x_train_tensor)``` returns predictions because how Python is built and works.\n",
    "\n",
    "The ```model()``` is instance of our class, that class inherited stuff from ```nn.Module``` and when we do ```yhat = model(x_train_tensor)```, Python looks for ```forward()``` in the class. If there is no ```forward()``` it will raise an error.\n",
    "\n",
    "**Remember:** You should make predictions that call model(x).\n",
    "\n",
    "Do **not** call ```model.forward(x)```!\n",
    "\n",
    "Otherwise, your model’s hooks will not work (if you have them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b72293",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = ManualLinearRegression().to(device)        # 1)\n",
    "\n",
    "# optimizer = optim.SGD([b, w], lr=lr) <- we did this before.\n",
    "# Now, since we wrapped them in Prameters, we just do:\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "n_epochs = 1000\n",
    "for epoch in range(n_epochs):\n",
    "    # set the model to training mode\n",
    "    model.train()                                # 2)\n",
    "\n",
    "    # Step 1 - computes model's predicted output - forward pass\n",
    "    # No more manual prediction!\n",
    "    yhat = model(x_train_tensor)                 # 3)\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step 4 - updates parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd5e156",
   "metadata": {},
   "source": [
    "It turns out, state dictionaries can also be used for checkpointing a model as we will see in the Rethinking the Training Loop chapter.\n",
    "\n",
    "\n",
    "# Nested v. Sequential Models\n",
    "\n",
    "PyTorch's built-in lienar regression model: 1 input, 1 output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f36432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1, out_features=1, bias=True)\n",
      "OrderedDict([('weight', tensor([[0.8692]])), ('bias', tensor([0.1872]))])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(1, 1)\n",
    "print(linear)\n",
    "print(linear.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac0f06",
   "metadata": {},
   "source": [
    "We can replace our manually created bias and weight in the class we created above by the built-in function of PyTorch.\n",
    "\n",
    "The ```linear``` model in our class is what they call nested model!\n",
    "\n",
    "-----------\n",
    "**Old Class**\n",
    "\n",
    "```python\n",
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.w = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.b + self.w * x\n",
    "```\n",
    "\n",
    "-----------\n",
    "In the ```__init__``` method, we create an attribute that contains our nested ```Linear``` model.\n",
    "\n",
    "In the ```forward()``` method, we call the nested model itself to perform the forward pass (notice that we are not calling ```self.linear.forward(x))```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae8c6b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.7645]], requires_grad=True), Parameter containing:\n",
      "tensor([0.8300], requires_grad=True)]\n",
      "------------------------------------------------------------------------------------------\n",
      "OrderedDict([('linear.weight', tensor([[0.7645]])), ('linear.bias', tensor([0.8300]))])\n"
     ]
    }
   ],
   "source": [
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear model \n",
    "        # with a single input and a single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call\n",
    "        self.linear(x)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dummy = MyLinearRegression().to(device)\n",
    "print(list(dummy.parameters()))\n",
    "print (\"-\"*90)\n",
    "print(dummy.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ada11",
   "metadata": {},
   "source": [
    "#### Sequential models\n",
    "\n",
    "For straightforward models that use a series of built-in PyTorch models (like ```Linear```) where the output of one is sequentially fed as an input to the next, we can use a ```Sequential``` model.\n",
    "\n",
    "In our case, we would build a ```Sequential``` model with a single argument; that is, the ```Linear``` model we used to train our linear regression. The model would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08cbea92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[0.7645]])), ('0.bias', tensor([0.8300]))])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(42)\n",
    "# Alternatively, you can use a Sequential model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce7a78",
   "metadata": {},
   "source": [
    "## Hold On. Zoom out:\n",
    "\n",
    "A nested model is ``nested'' because:\n",
    "- A module contains other modules\n",
    "- Those modules contain other modules\n",
    "- It forms a tree structure\n",
    "\n",
    "Sequential models are also technically nested! The difference is **control over the forward pass**.\n",
    "\n",
    "Nested:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    return self.output(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2570049",
   "metadata": {},
   "source": [
    "\n",
    "You can:\n",
    "- Add skip connections\n",
    "- Add branching\n",
    "- Use multiple inputs\n",
    "- Store intermediate outputs\n",
    "- Use conditionals\n",
    "- Reuse blocks\n",
    "- Loop dynamically\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x1 = self.block1(x)\n",
    "    x2 = self.block2(x1)\n",
    "    return self.output(x1 + x2)\n",
    "```\n",
    "\n",
    "```Sequential``` is basically output of one layer → input of next layer. No branching, no nothing:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    for module in self.modules:\n",
    "        x = module(x)\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df938362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (layer1): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (layer2): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Sequential model with naming layers:\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential()\n",
    "model.add_module('layer1', nn.Linear(3, 5))\n",
    "model.add_module('layer2', nn.Linear(5, 1))\n",
    "print(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876dfaa",
   "metadata": {},
   "source": [
    "Magic commands of Jupyter:\n",
    "\n",
    "```python\n",
    "%%writefile data_preparation/v0.py\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)\n",
    "\n",
    "\n",
    "%run -i data_preparation/v0.py\n",
    "```\n",
    "\n",
    "``` -i``` option to make all variables available from both the notebook and the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3898e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Model Config:\n",
    "###\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "###\n",
    "### Train:\n",
    "###\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    yhat = model(x_train_tensor)\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "    loss.backward() # Step 3 - computes gradients\n",
    "    # Step 4 - updates parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84e555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
